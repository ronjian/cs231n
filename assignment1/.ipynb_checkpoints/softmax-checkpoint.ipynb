{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.332193\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *Fill this in*\n",
    ">By random, the correct class account for around 0.1 of total 10 class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -1.058905 analytic: -1.058905, relative error: 3.598741e-08\n",
      "numerical: -3.095589 analytic: -3.095589, relative error: 2.526547e-08\n",
      "numerical: 3.314914 analytic: 3.314914, relative error: 7.047453e-09\n",
      "numerical: 2.477545 analytic: 2.477545, relative error: 5.548028e-09\n",
      "numerical: 1.732910 analytic: 1.732910, relative error: 6.247713e-09\n",
      "numerical: -3.011563 analytic: -3.011563, relative error: 1.299684e-08\n",
      "numerical: -0.649851 analytic: -0.649852, relative error: 1.241980e-07\n",
      "numerical: 1.700528 analytic: 1.700528, relative error: 9.666418e-09\n",
      "numerical: 1.712951 analytic: 1.712951, relative error: 3.352075e-08\n",
      "numerical: -0.315668 analytic: -0.315668, relative error: 1.156974e-07\n",
      "numerical: 1.149376 analytic: 1.149376, relative error: 3.209070e-08\n",
      "numerical: -0.778337 analytic: -0.778337, relative error: 5.989183e-08\n",
      "numerical: 1.557969 analytic: 1.557969, relative error: 1.068778e-08\n",
      "numerical: -1.427928 analytic: -1.427928, relative error: 4.868934e-08\n",
      "numerical: -1.407266 analytic: -1.407266, relative error: 2.180218e-08\n",
      "numerical: -5.514951 analytic: -5.514951, relative error: 4.334278e-09\n",
      "numerical: 0.340446 analytic: 0.340446, relative error: 6.707509e-08\n",
      "numerical: -0.257276 analytic: -0.257276, relative error: 1.873087e-07\n",
      "numerical: 0.743551 analytic: 0.743550, relative error: 4.890776e-08\n",
      "numerical: 1.086163 analytic: 1.086162, relative error: 8.278741e-08\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.332193e+00 computed in 0.173648s\n",
      "vectorized loss: 2.332193e+00 computed in 0.014287s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2e-08, 16000.0)\n",
      "iteration 0 / 5000: loss 497.905427\n",
      "iteration 100 / 5000: loss 437.919512\n",
      "iteration 200 / 5000: loss 384.869683\n",
      "iteration 300 / 5000: loss 338.492482\n",
      "iteration 400 / 5000: loss 297.563632\n",
      "iteration 500 / 5000: loss 262.063639\n",
      "iteration 600 / 5000: loss 230.614501\n",
      "iteration 700 / 5000: loss 203.036003\n",
      "iteration 800 / 5000: loss 178.612557\n",
      "iteration 900 / 5000: loss 157.390801\n",
      "iteration 1000 / 5000: loss 138.739730\n",
      "iteration 1100 / 5000: loss 122.253877\n",
      "iteration 1200 / 5000: loss 107.693167\n",
      "iteration 1300 / 5000: loss 95.084495\n",
      "iteration 1400 / 5000: loss 83.682973\n",
      "iteration 1500 / 5000: loss 73.922390\n",
      "iteration 1600 / 5000: loss 65.210764\n",
      "iteration 1700 / 5000: loss 57.576571\n",
      "iteration 1800 / 5000: loss 50.952231\n",
      "iteration 1900 / 5000: loss 45.089615\n",
      "iteration 2000 / 5000: loss 39.856790\n",
      "iteration 2100 / 5000: loss 35.287664\n",
      "iteration 2200 / 5000: loss 31.354886\n",
      "iteration 2300 / 5000: loss 27.710866\n",
      "iteration 2400 / 5000: loss 24.609091\n",
      "iteration 2500 / 5000: loss 21.916241\n",
      "iteration 2600 / 5000: loss 19.591323\n",
      "iteration 2700 / 5000: loss 17.436435\n",
      "iteration 2800 / 5000: loss 15.485160\n",
      "iteration 2900 / 5000: loss 13.932208\n",
      "iteration 3000 / 5000: loss 12.468312\n",
      "iteration 3100 / 5000: loss 11.184552\n",
      "iteration 3200 / 5000: loss 10.098618\n",
      "iteration 3300 / 5000: loss 9.225478\n",
      "iteration 3400 / 5000: loss 8.272185\n",
      "iteration 3500 / 5000: loss 7.497199\n",
      "iteration 3600 / 5000: loss 6.927223\n",
      "iteration 3700 / 5000: loss 6.311106\n",
      "iteration 3800 / 5000: loss 5.768751\n",
      "iteration 3900 / 5000: loss 5.304767\n",
      "iteration 4000 / 5000: loss 4.937218\n",
      "iteration 4100 / 5000: loss 4.640738\n",
      "iteration 4200 / 5000: loss 4.311102\n",
      "iteration 4300 / 5000: loss 4.088090\n",
      "iteration 4400 / 5000: loss 3.750668\n",
      "iteration 4500 / 5000: loss 3.528866\n",
      "iteration 4600 / 5000: loss 3.391812\n",
      "iteration 4700 / 5000: loss 3.244430\n",
      "iteration 4800 / 5000: loss 3.070647\n",
      "iteration 4900 / 5000: loss 2.878587\n",
      "(2e-08, 18000.0)\n",
      "iteration 0 / 5000: loss 553.093517\n",
      "iteration 100 / 5000: loss 478.835189\n",
      "iteration 200 / 5000: loss 413.528082\n",
      "iteration 300 / 5000: loss 358.052189\n",
      "iteration 400 / 5000: loss 310.113733\n",
      "iteration 500 / 5000: loss 268.327817\n",
      "iteration 600 / 5000: loss 232.816067\n",
      "iteration 700 / 5000: loss 201.410380\n",
      "iteration 800 / 5000: loss 174.770572\n",
      "iteration 900 / 5000: loss 151.470905\n",
      "iteration 1000 / 5000: loss 131.429722\n",
      "iteration 1100 / 5000: loss 113.981571\n",
      "iteration 1200 / 5000: loss 98.845893\n",
      "iteration 1300 / 5000: loss 85.761426\n",
      "iteration 1400 / 5000: loss 74.578533\n",
      "iteration 1500 / 5000: loss 64.781399\n",
      "iteration 1600 / 5000: loss 56.435323\n",
      "iteration 1700 / 5000: loss 49.011783\n",
      "iteration 1800 / 5000: loss 42.851239\n",
      "iteration 1900 / 5000: loss 37.290217\n",
      "iteration 2000 / 5000: loss 32.476085\n",
      "iteration 2100 / 5000: loss 28.432962\n",
      "iteration 2200 / 5000: loss 24.843804\n",
      "iteration 2300 / 5000: loss 21.726896\n",
      "iteration 2400 / 5000: loss 19.157479\n",
      "iteration 2500 / 5000: loss 16.826771\n",
      "iteration 2600 / 5000: loss 14.912418\n",
      "iteration 2700 / 5000: loss 13.106364\n",
      "iteration 2800 / 5000: loss 11.729497\n",
      "iteration 2900 / 5000: loss 10.259553\n",
      "iteration 3000 / 5000: loss 9.286372\n",
      "iteration 3100 / 5000: loss 8.265932\n",
      "iteration 3200 / 5000: loss 7.379495\n",
      "iteration 3300 / 5000: loss 6.740601\n",
      "iteration 3400 / 5000: loss 6.053425\n",
      "iteration 3500 / 5000: loss 5.571429\n",
      "iteration 3600 / 5000: loss 5.127944\n",
      "iteration 3700 / 5000: loss 4.760720\n",
      "iteration 3800 / 5000: loss 4.269620\n",
      "iteration 3900 / 5000: loss 4.030474\n",
      "iteration 4000 / 5000: loss 3.747804\n",
      "iteration 4100 / 5000: loss 3.492519\n",
      "iteration 4200 / 5000: loss 3.330113\n",
      "iteration 4300 / 5000: loss 3.137802\n",
      "iteration 4400 / 5000: loss 3.024140\n",
      "iteration 4500 / 5000: loss 2.873457\n",
      "iteration 4600 / 5000: loss 2.752124\n",
      "iteration 4700 / 5000: loss 2.691929\n",
      "iteration 4800 / 5000: loss 2.576987\n",
      "iteration 4900 / 5000: loss 2.522062\n",
      "(2e-08, 20000.0)\n",
      "iteration 0 / 5000: loss 625.408423\n",
      "iteration 100 / 5000: loss 532.875309\n",
      "iteration 200 / 5000: loss 453.899002\n",
      "iteration 300 / 5000: loss 386.774433\n",
      "iteration 400 / 5000: loss 329.685446\n",
      "iteration 500 / 5000: loss 280.871120\n",
      "iteration 600 / 5000: loss 239.563532\n",
      "iteration 700 / 5000: loss 204.278097\n",
      "iteration 800 / 5000: loss 174.158753\n",
      "iteration 900 / 5000: loss 148.640290\n",
      "iteration 1000 / 5000: loss 126.941810\n",
      "iteration 1100 / 5000: loss 108.239975\n",
      "iteration 1200 / 5000: loss 92.451387\n",
      "iteration 1300 / 5000: loss 79.242748\n",
      "iteration 1400 / 5000: loss 67.614136\n",
      "iteration 1500 / 5000: loss 57.980732\n",
      "iteration 1600 / 5000: loss 49.719989\n",
      "iteration 1700 / 5000: loss 42.587844\n",
      "iteration 1800 / 5000: loss 36.551172\n",
      "iteration 1900 / 5000: loss 31.458915\n",
      "iteration 2000 / 5000: loss 27.042451\n",
      "iteration 2100 / 5000: loss 23.371875\n",
      "iteration 2200 / 5000: loss 20.230037\n",
      "iteration 2300 / 5000: loss 17.523177\n",
      "iteration 2400 / 5000: loss 15.268792\n",
      "iteration 2500 / 5000: loss 13.276411\n",
      "iteration 2600 / 5000: loss 11.622378\n",
      "iteration 2700 / 5000: loss 10.266684\n",
      "iteration 2800 / 5000: loss 8.952346\n",
      "iteration 2900 / 5000: loss 7.972131\n",
      "iteration 3000 / 5000: loss 7.090104\n",
      "iteration 3100 / 5000: loss 6.330250\n",
      "iteration 3200 / 5000: loss 5.781022\n",
      "iteration 3300 / 5000: loss 5.252069\n",
      "iteration 3400 / 5000: loss 4.730016\n",
      "iteration 3500 / 5000: loss 4.350046\n",
      "iteration 3600 / 5000: loss 3.995757\n",
      "iteration 3700 / 5000: loss 3.680913\n",
      "iteration 3800 / 5000: loss 3.506978\n",
      "iteration 3900 / 5000: loss 3.252234\n",
      "iteration 4000 / 5000: loss 3.023752\n",
      "iteration 4100 / 5000: loss 2.900872\n",
      "iteration 4200 / 5000: loss 2.719335\n",
      "iteration 4300 / 5000: loss 2.678887\n",
      "iteration 4400 / 5000: loss 2.596115\n",
      "iteration 4500 / 5000: loss 2.518756\n",
      "iteration 4600 / 5000: loss 2.431105\n",
      "iteration 4700 / 5000: loss 2.385588\n",
      "iteration 4800 / 5000: loss 2.340859\n",
      "iteration 4900 / 5000: loss 2.236925\n",
      "(2e-08, 23000.0)\n",
      "iteration 0 / 5000: loss 717.500767\n",
      "iteration 100 / 5000: loss 596.563049\n",
      "iteration 200 / 5000: loss 495.999713\n",
      "iteration 300 / 5000: loss 412.556255\n",
      "iteration 400 / 5000: loss 343.462301\n",
      "iteration 500 / 5000: loss 285.699218\n",
      "iteration 600 / 5000: loss 238.056717\n",
      "iteration 700 / 5000: loss 198.088413\n",
      "iteration 800 / 5000: loss 165.163522\n",
      "iteration 900 / 5000: loss 137.565935\n",
      "iteration 1000 / 5000: loss 114.827942\n",
      "iteration 1100 / 5000: loss 95.733590\n",
      "iteration 1200 / 5000: loss 79.950782\n",
      "iteration 1300 / 5000: loss 66.919982\n",
      "iteration 1400 / 5000: loss 55.935072\n",
      "iteration 1500 / 5000: loss 46.864943\n",
      "iteration 1600 / 5000: loss 39.354424\n",
      "iteration 1700 / 5000: loss 32.980781\n",
      "iteration 1800 / 5000: loss 27.837109\n",
      "iteration 1900 / 5000: loss 23.452087\n",
      "iteration 2000 / 5000: loss 19.886218\n",
      "iteration 2100 / 5000: loss 16.838475\n",
      "iteration 2200 / 5000: loss 14.350651\n",
      "iteration 2300 / 5000: loss 12.256921\n",
      "iteration 2400 / 5000: loss 10.587980\n",
      "iteration 2500 / 5000: loss 9.195298\n",
      "iteration 2600 / 5000: loss 7.908773\n",
      "iteration 2700 / 5000: loss 7.005263\n",
      "iteration 2800 / 5000: loss 6.104610\n",
      "iteration 2900 / 5000: loss 5.509537\n",
      "iteration 3000 / 5000: loss 4.902666\n",
      "iteration 3100 / 5000: loss 4.427543\n",
      "iteration 3200 / 5000: loss 4.039360\n",
      "iteration 3300 / 5000: loss 3.676738\n",
      "iteration 3400 / 5000: loss 3.393795\n",
      "iteration 3500 / 5000: loss 3.220712\n",
      "iteration 3600 / 5000: loss 2.986889\n",
      "iteration 3700 / 5000: loss 2.778592\n",
      "iteration 3800 / 5000: loss 2.695768\n",
      "iteration 3900 / 5000: loss 2.619084\n",
      "iteration 4000 / 5000: loss 2.479988\n",
      "iteration 4100 / 5000: loss 2.464820\n",
      "iteration 4200 / 5000: loss 2.346211\n",
      "iteration 4300 / 5000: loss 2.410005\n",
      "iteration 4400 / 5000: loss 2.338810\n",
      "iteration 4500 / 5000: loss 2.238185\n",
      "iteration 4600 / 5000: loss 2.217605\n",
      "iteration 4700 / 5000: loss 2.177486\n",
      "iteration 4800 / 5000: loss 2.242690\n",
      "iteration 4900 / 5000: loss 2.162991\n",
      "(2e-08, 26000.0)\n",
      "iteration 0 / 5000: loss 802.914861\n",
      "iteration 100 / 5000: loss 650.965174\n",
      "iteration 200 / 5000: loss 528.991359\n",
      "iteration 300 / 5000: loss 429.471198\n",
      "iteration 400 / 5000: loss 348.615225\n",
      "iteration 500 / 5000: loss 283.373131\n",
      "iteration 600 / 5000: loss 230.418311\n",
      "iteration 700 / 5000: loss 187.279957\n",
      "iteration 800 / 5000: loss 152.579728\n",
      "iteration 900 / 5000: loss 124.226022\n",
      "iteration 1000 / 5000: loss 101.153690\n",
      "iteration 1100 / 5000: loss 82.638221\n",
      "iteration 1200 / 5000: loss 67.257634\n",
      "iteration 1300 / 5000: loss 55.057012\n",
      "iteration 1400 / 5000: loss 45.164996\n",
      "iteration 1500 / 5000: loss 37.007600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1600 / 5000: loss 30.405250\n",
      "iteration 1700 / 5000: loss 25.093656\n",
      "iteration 1800 / 5000: loss 20.705496\n",
      "iteration 1900 / 5000: loss 17.258902\n",
      "iteration 2000 / 5000: loss 14.400641\n",
      "iteration 2100 / 5000: loss 12.028942\n",
      "iteration 2200 / 5000: loss 10.200793\n",
      "iteration 2300 / 5000: loss 8.640238\n",
      "iteration 2400 / 5000: loss 7.436929\n",
      "iteration 2500 / 5000: loss 6.431831\n",
      "iteration 2600 / 5000: loss 5.619967\n",
      "iteration 2700 / 5000: loss 4.960652\n",
      "iteration 2800 / 5000: loss 4.439472\n",
      "iteration 2900 / 5000: loss 4.015490\n",
      "iteration 3000 / 5000: loss 3.612673\n",
      "iteration 3100 / 5000: loss 3.307690\n",
      "iteration 3200 / 5000: loss 3.104804\n",
      "iteration 3300 / 5000: loss 2.850699\n",
      "iteration 3400 / 5000: loss 2.769531\n",
      "iteration 3500 / 5000: loss 2.592401\n",
      "iteration 3600 / 5000: loss 2.518591\n",
      "iteration 3700 / 5000: loss 2.421675\n",
      "iteration 3800 / 5000: loss 2.395152\n",
      "iteration 3900 / 5000: loss 2.341800\n",
      "iteration 4000 / 5000: loss 2.306297\n",
      "iteration 4100 / 5000: loss 2.246045\n",
      "iteration 4200 / 5000: loss 2.206137\n",
      "iteration 4300 / 5000: loss 2.160600\n",
      "iteration 4400 / 5000: loss 2.118934\n",
      "iteration 4500 / 5000: loss 2.198769\n",
      "iteration 4600 / 5000: loss 2.127511\n",
      "iteration 4700 / 5000: loss 2.107947\n",
      "iteration 4800 / 5000: loss 2.144523\n",
      "iteration 4900 / 5000: loss 2.123152\n",
      "(3e-08, 16000.0)\n",
      "iteration 0 / 5000: loss 501.997563\n",
      "iteration 100 / 5000: loss 413.701499\n",
      "iteration 200 / 5000: loss 341.308159\n",
      "iteration 300 / 5000: loss 281.773738\n",
      "iteration 400 / 5000: loss 232.370574\n",
      "iteration 500 / 5000: loss 191.947593\n",
      "iteration 600 / 5000: loss 158.612387\n",
      "iteration 700 / 5000: loss 131.221596\n",
      "iteration 800 / 5000: loss 108.500105\n",
      "iteration 900 / 5000: loss 89.699362\n",
      "iteration 1000 / 5000: loss 74.315160\n",
      "iteration 1100 / 5000: loss 61.655732\n",
      "iteration 1200 / 5000: loss 51.273304\n",
      "iteration 1300 / 5000: loss 42.660069\n",
      "iteration 1400 / 5000: loss 35.464774\n",
      "iteration 1500 / 5000: loss 29.625578\n",
      "iteration 1600 / 5000: loss 24.774820\n",
      "iteration 1700 / 5000: loss 20.807561\n",
      "iteration 1800 / 5000: loss 17.507434\n",
      "iteration 1900 / 5000: loss 14.826645\n",
      "iteration 2000 / 5000: loss 12.501418\n",
      "iteration 2100 / 5000: loss 10.742256\n",
      "iteration 2200 / 5000: loss 9.210560\n",
      "iteration 2300 / 5000: loss 7.921636\n",
      "iteration 2400 / 5000: loss 6.945453\n",
      "iteration 2500 / 5000: loss 5.977097\n",
      "iteration 2600 / 5000: loss 5.363079\n",
      "iteration 2700 / 5000: loss 4.828757\n",
      "iteration 2800 / 5000: loss 4.343654\n",
      "iteration 2900 / 5000: loss 3.888433\n",
      "iteration 3000 / 5000: loss 3.580451\n",
      "iteration 3100 / 5000: loss 3.273595\n",
      "iteration 3200 / 5000: loss 3.028400\n",
      "iteration 3300 / 5000: loss 2.902003\n",
      "iteration 3400 / 5000: loss 2.762553\n",
      "iteration 3500 / 5000: loss 2.644631\n",
      "iteration 3600 / 5000: loss 2.564931\n",
      "iteration 3700 / 5000: loss 2.454802\n",
      "iteration 3800 / 5000: loss 2.427078\n",
      "iteration 3900 / 5000: loss 2.328578\n",
      "iteration 4000 / 5000: loss 2.199433\n",
      "iteration 4100 / 5000: loss 2.260572\n",
      "iteration 4200 / 5000: loss 2.221871\n",
      "iteration 4300 / 5000: loss 2.199560\n",
      "iteration 4400 / 5000: loss 2.168784\n",
      "iteration 4500 / 5000: loss 2.120514\n",
      "iteration 4600 / 5000: loss 2.131327\n",
      "iteration 4700 / 5000: loss 2.043816\n",
      "iteration 4800 / 5000: loss 2.094569\n",
      "iteration 4900 / 5000: loss 2.053903\n",
      "(3e-08, 18000.0)\n",
      "iteration 0 / 5000: loss 550.619379\n",
      "iteration 100 / 5000: loss 443.305939\n",
      "iteration 200 / 5000: loss 356.485600\n",
      "iteration 300 / 5000: loss 287.375505\n",
      "iteration 400 / 5000: loss 231.414252\n",
      "iteration 500 / 5000: loss 186.635572\n",
      "iteration 600 / 5000: loss 150.823493\n",
      "iteration 700 / 5000: loss 121.796353\n",
      "iteration 800 / 5000: loss 98.426435\n",
      "iteration 900 / 5000: loss 79.618193\n",
      "iteration 1000 / 5000: loss 64.520738\n",
      "iteration 1100 / 5000: loss 52.302286\n",
      "iteration 1200 / 5000: loss 42.493603\n",
      "iteration 1300 / 5000: loss 34.631566\n",
      "iteration 1400 / 5000: loss 28.376604\n",
      "iteration 1500 / 5000: loss 23.188620\n",
      "iteration 1600 / 5000: loss 19.072059\n",
      "iteration 1700 / 5000: loss 15.762316\n",
      "iteration 1800 / 5000: loss 13.144136\n",
      "iteration 1900 / 5000: loss 10.924847\n",
      "iteration 2000 / 5000: loss 9.180089\n",
      "iteration 2100 / 5000: loss 7.833182\n",
      "iteration 2200 / 5000: loss 6.726501\n",
      "iteration 2300 / 5000: loss 5.762009\n",
      "iteration 2400 / 5000: loss 5.072593\n",
      "iteration 2500 / 5000: loss 4.519804\n",
      "iteration 2600 / 5000: loss 4.012893\n",
      "iteration 2700 / 5000: loss 3.633588\n",
      "iteration 2800 / 5000: loss 3.322467\n",
      "iteration 2900 / 5000: loss 3.123737\n",
      "iteration 3000 / 5000: loss 2.825371\n",
      "iteration 3100 / 5000: loss 2.715746\n",
      "iteration 3200 / 5000: loss 2.527961\n",
      "iteration 3300 / 5000: loss 2.433340\n",
      "iteration 3400 / 5000: loss 2.441312\n",
      "iteration 3500 / 5000: loss 2.367004\n",
      "iteration 3600 / 5000: loss 2.286952\n",
      "iteration 3700 / 5000: loss 2.294557\n",
      "iteration 3800 / 5000: loss 2.234357\n",
      "iteration 3900 / 5000: loss 2.179989\n",
      "iteration 4000 / 5000: loss 2.133618\n",
      "iteration 4100 / 5000: loss 2.143517\n",
      "iteration 4200 / 5000: loss 2.107375\n",
      "iteration 4300 / 5000: loss 2.071803\n",
      "iteration 4400 / 5000: loss 2.100385\n",
      "iteration 4500 / 5000: loss 2.061524\n",
      "iteration 4600 / 5000: loss 2.011925\n",
      "iteration 4700 / 5000: loss 2.102092\n",
      "iteration 4800 / 5000: loss 2.010207\n",
      "iteration 4900 / 5000: loss 2.031365\n",
      "(3e-08, 20000.0)\n",
      "iteration 0 / 5000: loss 622.751244\n",
      "iteration 100 / 5000: loss 488.928375\n",
      "iteration 200 / 5000: loss 384.233682\n",
      "iteration 300 / 5000: loss 302.519924\n",
      "iteration 400 / 5000: loss 238.033865\n",
      "iteration 500 / 5000: loss 187.528312\n",
      "iteration 600 / 5000: loss 147.784357\n",
      "iteration 700 / 5000: loss 116.562914\n",
      "iteration 800 / 5000: loss 92.100044\n",
      "iteration 900 / 5000: loss 72.746101\n",
      "iteration 1000 / 5000: loss 57.588803\n",
      "iteration 1100 / 5000: loss 45.812648\n",
      "iteration 1200 / 5000: loss 36.403447\n",
      "iteration 1300 / 5000: loss 29.074233\n",
      "iteration 1400 / 5000: loss 23.292338\n",
      "iteration 1500 / 5000: loss 18.682431\n",
      "iteration 1600 / 5000: loss 15.159536\n",
      "iteration 1700 / 5000: loss 12.341196\n",
      "iteration 1800 / 5000: loss 10.170969\n",
      "iteration 1900 / 5000: loss 8.423822\n",
      "iteration 2000 / 5000: loss 7.105807\n",
      "iteration 2100 / 5000: loss 5.986128\n",
      "iteration 2200 / 5000: loss 5.167403\n",
      "iteration 2300 / 5000: loss 4.491830\n",
      "iteration 2400 / 5000: loss 3.964508\n",
      "iteration 2500 / 5000: loss 3.504378\n",
      "iteration 2600 / 5000: loss 3.235036\n",
      "iteration 2700 / 5000: loss 3.023447\n",
      "iteration 2800 / 5000: loss 2.754306\n",
      "iteration 2900 / 5000: loss 2.606296\n",
      "iteration 3000 / 5000: loss 2.493170\n",
      "iteration 3100 / 5000: loss 2.429996\n",
      "iteration 3200 / 5000: loss 2.343593\n",
      "iteration 3300 / 5000: loss 2.278843\n",
      "iteration 3400 / 5000: loss 2.193505\n",
      "iteration 3500 / 5000: loss 2.171917\n",
      "iteration 3600 / 5000: loss 2.204584\n",
      "iteration 3700 / 5000: loss 2.152304\n",
      "iteration 3800 / 5000: loss 2.102274\n",
      "iteration 3900 / 5000: loss 2.182844\n",
      "iteration 4000 / 5000: loss 2.054146\n",
      "iteration 4100 / 5000: loss 2.073176\n",
      "iteration 4200 / 5000: loss 2.134008\n",
      "iteration 4300 / 5000: loss 2.110849\n",
      "iteration 4400 / 5000: loss 2.068738\n",
      "iteration 4500 / 5000: loss 2.115213\n",
      "iteration 4600 / 5000: loss 2.053274\n",
      "iteration 4700 / 5000: loss 2.081845\n",
      "iteration 4800 / 5000: loss 2.141401\n",
      "iteration 4900 / 5000: loss 2.041071\n",
      "(3e-08, 23000.0)\n",
      "iteration 0 / 5000: loss 715.896981\n",
      "iteration 100 / 5000: loss 542.751675\n",
      "iteration 200 / 5000: loss 411.400454\n",
      "iteration 300 / 5000: loss 312.232421\n",
      "iteration 400 / 5000: loss 237.161571\n",
      "iteration 500 / 5000: loss 180.339945\n",
      "iteration 600 / 5000: loss 137.193259\n",
      "iteration 700 / 5000: loss 104.530288\n",
      "iteration 800 / 5000: loss 79.711794\n",
      "iteration 900 / 5000: loss 60.920708\n",
      "iteration 1000 / 5000: loss 46.744529\n",
      "iteration 1100 / 5000: loss 35.905591\n",
      "iteration 1200 / 5000: loss 27.697702\n",
      "iteration 1300 / 5000: loss 21.498840\n",
      "iteration 1400 / 5000: loss 16.836293\n",
      "iteration 1500 / 5000: loss 13.244514\n",
      "iteration 1600 / 5000: loss 10.518767\n",
      "iteration 1700 / 5000: loss 8.530047\n",
      "iteration 1800 / 5000: loss 6.929049\n",
      "iteration 1900 / 5000: loss 5.775658\n",
      "iteration 2000 / 5000: loss 4.891335\n",
      "iteration 2100 / 5000: loss 4.240387\n",
      "iteration 2200 / 5000: loss 3.721144\n",
      "iteration 2300 / 5000: loss 3.265762\n",
      "iteration 2400 / 5000: loss 3.012994\n",
      "iteration 2500 / 5000: loss 2.804741\n",
      "iteration 2600 / 5000: loss 2.582099\n",
      "iteration 2700 / 5000: loss 2.494974\n",
      "iteration 2800 / 5000: loss 2.393488\n",
      "iteration 2900 / 5000: loss 2.269821\n",
      "iteration 3000 / 5000: loss 2.263816\n",
      "iteration 3100 / 5000: loss 2.185340\n",
      "iteration 3200 / 5000: loss 2.138088\n",
      "iteration 3300 / 5000: loss 2.107360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3400 / 5000: loss 2.124925\n",
      "iteration 3500 / 5000: loss 2.102493\n",
      "iteration 3600 / 5000: loss 2.134035\n",
      "iteration 3700 / 5000: loss 2.109470\n",
      "iteration 3800 / 5000: loss 2.085263\n",
      "iteration 3900 / 5000: loss 2.087902\n",
      "iteration 4000 / 5000: loss 2.184806\n",
      "iteration 4100 / 5000: loss 2.081473\n",
      "iteration 4200 / 5000: loss 2.170680\n",
      "iteration 4300 / 5000: loss 2.110029\n",
      "iteration 4400 / 5000: loss 2.085301\n",
      "iteration 4500 / 5000: loss 2.043043\n",
      "iteration 4600 / 5000: loss 2.079741\n",
      "iteration 4700 / 5000: loss 2.080897\n",
      "iteration 4800 / 5000: loss 2.024361\n",
      "iteration 4900 / 5000: loss 2.088658\n",
      "(3e-08, 26000.0)\n",
      "iteration 0 / 5000: loss 812.861199\n",
      "iteration 100 / 5000: loss 595.038183\n",
      "iteration 200 / 5000: loss 435.038078\n",
      "iteration 300 / 5000: loss 318.752647\n",
      "iteration 400 / 5000: loss 233.752909\n",
      "iteration 500 / 5000: loss 171.437373\n",
      "iteration 600 / 5000: loss 125.874489\n",
      "iteration 700 / 5000: loss 92.611223\n",
      "iteration 800 / 5000: loss 68.332040\n",
      "iteration 900 / 5000: loss 50.544013\n",
      "iteration 1000 / 5000: loss 37.416124\n",
      "iteration 1100 / 5000: loss 27.970488\n",
      "iteration 1200 / 5000: loss 21.041490\n",
      "iteration 1300 / 5000: loss 15.980036\n",
      "iteration 1400 / 5000: loss 12.295349\n",
      "iteration 1500 / 5000: loss 9.508573\n",
      "iteration 1600 / 5000: loss 7.498524\n",
      "iteration 1700 / 5000: loss 6.017152\n",
      "iteration 1800 / 5000: loss 4.968852\n",
      "iteration 1900 / 5000: loss 4.226115\n",
      "iteration 2000 / 5000: loss 3.609747\n",
      "iteration 2100 / 5000: loss 3.248703\n",
      "iteration 2200 / 5000: loss 2.984106\n",
      "iteration 2300 / 5000: loss 2.674024\n",
      "iteration 2400 / 5000: loss 2.551143\n",
      "iteration 2500 / 5000: loss 2.416438\n",
      "iteration 2600 / 5000: loss 2.311086\n",
      "iteration 2700 / 5000: loss 2.261349\n",
      "iteration 2800 / 5000: loss 2.195928\n",
      "iteration 2900 / 5000: loss 2.140934\n",
      "iteration 3000 / 5000: loss 2.154877\n",
      "iteration 3100 / 5000: loss 2.074525\n",
      "iteration 3200 / 5000: loss 2.115636\n",
      "iteration 3300 / 5000: loss 2.052786\n",
      "iteration 3400 / 5000: loss 2.115880\n",
      "iteration 3500 / 5000: loss 2.135274\n",
      "iteration 3600 / 5000: loss 2.108034\n",
      "iteration 3700 / 5000: loss 2.031493\n",
      "iteration 3800 / 5000: loss 2.109699\n",
      "iteration 3900 / 5000: loss 2.145363\n",
      "iteration 4000 / 5000: loss 2.121800\n",
      "iteration 4100 / 5000: loss 2.069302\n",
      "iteration 4200 / 5000: loss 2.100181\n",
      "iteration 4300 / 5000: loss 2.131482\n",
      "iteration 4400 / 5000: loss 2.097750\n",
      "iteration 4500 / 5000: loss 2.081144\n",
      "iteration 4600 / 5000: loss 2.036918\n",
      "iteration 4700 / 5000: loss 2.091632\n",
      "iteration 4800 / 5000: loss 2.132003\n",
      "iteration 4900 / 5000: loss 2.076022\n",
      "(4e-08, 16000.0)\n",
      "iteration 0 / 5000: loss 493.723648\n",
      "iteration 100 / 5000: loss 381.327080\n",
      "iteration 200 / 5000: loss 295.045877\n",
      "iteration 300 / 5000: loss 228.551091\n",
      "iteration 400 / 5000: loss 177.037147\n",
      "iteration 500 / 5000: loss 137.316280\n",
      "iteration 600 / 5000: loss 106.680422\n",
      "iteration 700 / 5000: loss 82.976112\n",
      "iteration 800 / 5000: loss 64.590649\n",
      "iteration 900 / 5000: loss 50.405342\n",
      "iteration 1000 / 5000: loss 39.546761\n",
      "iteration 1100 / 5000: loss 30.966574\n",
      "iteration 1200 / 5000: loss 24.467336\n",
      "iteration 1300 / 5000: loss 19.356396\n",
      "iteration 1400 / 5000: loss 15.454277\n",
      "iteration 1500 / 5000: loss 12.401499\n",
      "iteration 1600 / 5000: loss 10.047850\n",
      "iteration 1700 / 5000: loss 8.276729\n",
      "iteration 1800 / 5000: loss 6.823183\n",
      "iteration 1900 / 5000: loss 5.755202\n",
      "iteration 2000 / 5000: loss 4.912037\n",
      "iteration 2100 / 5000: loss 4.245816\n",
      "iteration 2200 / 5000: loss 3.746042\n",
      "iteration 2300 / 5000: loss 3.406480\n",
      "iteration 2400 / 5000: loss 3.020915\n",
      "iteration 2500 / 5000: loss 2.777822\n",
      "iteration 2600 / 5000: loss 2.654213\n",
      "iteration 2700 / 5000: loss 2.570561\n",
      "iteration 2800 / 5000: loss 2.421379\n",
      "iteration 2900 / 5000: loss 2.335426\n",
      "iteration 3000 / 5000: loss 2.200255\n",
      "iteration 3100 / 5000: loss 2.226046\n",
      "iteration 3200 / 5000: loss 2.185815\n",
      "iteration 3300 / 5000: loss 2.077533\n",
      "iteration 3400 / 5000: loss 2.098027\n",
      "iteration 3500 / 5000: loss 2.179079\n",
      "iteration 3600 / 5000: loss 2.099381\n",
      "iteration 3700 / 5000: loss 2.043592\n",
      "iteration 3800 / 5000: loss 2.087498\n",
      "iteration 3900 / 5000: loss 2.069861\n",
      "iteration 4000 / 5000: loss 2.075659\n",
      "iteration 4100 / 5000: loss 2.072572\n",
      "iteration 4200 / 5000: loss 2.091582\n",
      "iteration 4300 / 5000: loss 2.015992\n",
      "iteration 4400 / 5000: loss 1.998819\n",
      "iteration 4500 / 5000: loss 2.022222\n",
      "iteration 4600 / 5000: loss 2.026883\n",
      "iteration 4700 / 5000: loss 2.012947\n",
      "iteration 4800 / 5000: loss 2.005691\n",
      "iteration 4900 / 5000: loss 2.016602\n",
      "(4e-08, 18000.0)\n",
      "iteration 0 / 5000: loss 556.360324\n",
      "iteration 100 / 5000: loss 416.120284\n",
      "iteration 200 / 5000: loss 311.898691\n",
      "iteration 300 / 5000: loss 233.949237\n",
      "iteration 400 / 5000: loss 175.830289\n",
      "iteration 500 / 5000: loss 132.103609\n",
      "iteration 600 / 5000: loss 99.488917\n",
      "iteration 700 / 5000: loss 74.892762\n",
      "iteration 800 / 5000: loss 56.703996\n",
      "iteration 900 / 5000: loss 43.022693\n",
      "iteration 1000 / 5000: loss 32.656439\n",
      "iteration 1100 / 5000: loss 24.922535\n",
      "iteration 1200 / 5000: loss 19.247343\n",
      "iteration 1300 / 5000: loss 14.912050\n",
      "iteration 1400 / 5000: loss 11.650514\n",
      "iteration 1500 / 5000: loss 9.308944\n",
      "iteration 1600 / 5000: loss 7.502335\n",
      "iteration 1700 / 5000: loss 6.084272\n",
      "iteration 1800 / 5000: loss 5.118212\n",
      "iteration 1900 / 5000: loss 4.352904\n",
      "iteration 2000 / 5000: loss 3.697575\n",
      "iteration 2100 / 5000: loss 3.321497\n",
      "iteration 2200 / 5000: loss 3.007447\n",
      "iteration 2300 / 5000: loss 2.764002\n",
      "iteration 2400 / 5000: loss 2.604592\n",
      "iteration 2500 / 5000: loss 2.476689\n",
      "iteration 2600 / 5000: loss 2.285283\n",
      "iteration 2700 / 5000: loss 2.308126\n",
      "iteration 2800 / 5000: loss 2.179172\n",
      "iteration 2900 / 5000: loss 2.169194\n",
      "iteration 3000 / 5000: loss 2.141481\n",
      "iteration 3100 / 5000: loss 2.192026\n",
      "iteration 3200 / 5000: loss 2.052619\n",
      "iteration 3300 / 5000: loss 2.079959\n",
      "iteration 3400 / 5000: loss 2.065460\n",
      "iteration 3500 / 5000: loss 2.081097\n",
      "iteration 3600 / 5000: loss 2.042494\n",
      "iteration 3700 / 5000: loss 2.048881\n",
      "iteration 3800 / 5000: loss 2.102843\n",
      "iteration 3900 / 5000: loss 2.076131\n",
      "iteration 4000 / 5000: loss 2.099628\n",
      "iteration 4100 / 5000: loss 2.084967\n",
      "iteration 4200 / 5000: loss 2.073851\n",
      "iteration 4300 / 5000: loss 2.039774\n",
      "iteration 4400 / 5000: loss 2.053020\n",
      "iteration 4500 / 5000: loss 2.002779\n",
      "iteration 4600 / 5000: loss 2.032548\n",
      "iteration 4700 / 5000: loss 2.065395\n",
      "iteration 4800 / 5000: loss 2.091012\n",
      "iteration 4900 / 5000: loss 2.025177\n",
      "(4e-08, 20000.0)\n",
      "iteration 0 / 5000: loss 615.457727\n",
      "iteration 100 / 5000: loss 446.100528\n",
      "iteration 200 / 5000: loss 324.266901\n",
      "iteration 300 / 5000: loss 235.513986\n",
      "iteration 400 / 5000: loss 171.373313\n",
      "iteration 500 / 5000: loss 124.943077\n",
      "iteration 600 / 5000: loss 91.195348\n",
      "iteration 700 / 5000: loss 66.713624\n",
      "iteration 800 / 5000: loss 48.890726\n",
      "iteration 900 / 5000: loss 36.050197\n",
      "iteration 1000 / 5000: loss 26.734186\n",
      "iteration 1100 / 5000: loss 19.992453\n",
      "iteration 1200 / 5000: loss 15.085151\n",
      "iteration 1300 / 5000: loss 11.465283\n",
      "iteration 1400 / 5000: loss 8.903981\n",
      "iteration 1500 / 5000: loss 6.983752\n",
      "iteration 1600 / 5000: loss 5.659381\n",
      "iteration 1700 / 5000: loss 4.655674\n",
      "iteration 1800 / 5000: loss 3.986679\n",
      "iteration 1900 / 5000: loss 3.385952\n",
      "iteration 2000 / 5000: loss 3.043100\n",
      "iteration 2100 / 5000: loss 2.778145\n",
      "iteration 2200 / 5000: loss 2.592642\n",
      "iteration 2300 / 5000: loss 2.413402\n",
      "iteration 2400 / 5000: loss 2.345483\n",
      "iteration 2500 / 5000: loss 2.206054\n",
      "iteration 2600 / 5000: loss 2.186361\n",
      "iteration 2700 / 5000: loss 2.151087\n",
      "iteration 2800 / 5000: loss 2.145716\n",
      "iteration 2900 / 5000: loss 2.146223\n",
      "iteration 3000 / 5000: loss 2.099254\n",
      "iteration 3100 / 5000: loss 2.060609\n",
      "iteration 3200 / 5000: loss 2.140680\n",
      "iteration 3300 / 5000: loss 2.066957\n",
      "iteration 3400 / 5000: loss 2.120902\n",
      "iteration 3500 / 5000: loss 2.023370\n",
      "iteration 3600 / 5000: loss 2.128761\n",
      "iteration 3700 / 5000: loss 2.108751\n",
      "iteration 3800 / 5000: loss 2.063767\n",
      "iteration 3900 / 5000: loss 2.035504\n",
      "iteration 4000 / 5000: loss 2.064786\n",
      "iteration 4100 / 5000: loss 2.065141\n",
      "iteration 4200 / 5000: loss 2.020428\n",
      "iteration 4300 / 5000: loss 2.075619\n",
      "iteration 4400 / 5000: loss 2.071506\n",
      "iteration 4500 / 5000: loss 2.062536\n",
      "iteration 4600 / 5000: loss 2.057223\n",
      "iteration 4700 / 5000: loss 2.089059\n",
      "iteration 4800 / 5000: loss 2.026208\n",
      "iteration 4900 / 5000: loss 2.006393\n",
      "(4e-08, 23000.0)\n",
      "iteration 0 / 5000: loss 712.444472\n",
      "iteration 100 / 5000: loss 492.329319\n",
      "iteration 200 / 5000: loss 340.732108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 300 / 5000: loss 236.223844\n",
      "iteration 400 / 5000: loss 163.789878\n",
      "iteration 500 / 5000: loss 113.912808\n",
      "iteration 600 / 5000: loss 79.319095\n",
      "iteration 700 / 5000: loss 55.346349\n",
      "iteration 800 / 5000: loss 38.983736\n",
      "iteration 900 / 5000: loss 27.619224\n",
      "iteration 1000 / 5000: loss 19.741779\n",
      "iteration 1100 / 5000: loss 14.183785\n",
      "iteration 1200 / 5000: loss 10.508601\n",
      "iteration 1300 / 5000: loss 7.950449\n",
      "iteration 1400 / 5000: loss 6.108507\n",
      "iteration 1500 / 5000: loss 4.872092\n",
      "iteration 1600 / 5000: loss 3.945782\n",
      "iteration 1700 / 5000: loss 3.388264\n",
      "iteration 1800 / 5000: loss 3.035259\n",
      "iteration 1900 / 5000: loss 2.692811\n",
      "iteration 2000 / 5000: loss 2.517255\n",
      "iteration 2100 / 5000: loss 2.406826\n",
      "iteration 2200 / 5000: loss 2.367632\n",
      "iteration 2300 / 5000: loss 2.210166\n",
      "iteration 2400 / 5000: loss 2.192230\n",
      "iteration 2500 / 5000: loss 2.056289\n",
      "iteration 2600 / 5000: loss 2.047477\n",
      "iteration 2700 / 5000: loss 2.035502\n",
      "iteration 2800 / 5000: loss 2.109088\n",
      "iteration 2900 / 5000: loss 2.099669\n",
      "iteration 3000 / 5000: loss 2.082811\n",
      "iteration 3100 / 5000: loss 2.103732\n",
      "iteration 3200 / 5000: loss 2.025731\n",
      "iteration 3300 / 5000: loss 2.078862\n",
      "iteration 3400 / 5000: loss 2.053619\n",
      "iteration 3500 / 5000: loss 2.141257\n",
      "iteration 3600 / 5000: loss 2.017351\n",
      "iteration 3700 / 5000: loss 2.151971\n",
      "iteration 3800 / 5000: loss 2.069674\n",
      "iteration 3900 / 5000: loss 2.036700\n",
      "iteration 4000 / 5000: loss 2.127860\n",
      "iteration 4100 / 5000: loss 2.062656\n",
      "iteration 4200 / 5000: loss 2.094205\n",
      "iteration 4300 / 5000: loss 2.060194\n",
      "iteration 4400 / 5000: loss 2.093511\n",
      "iteration 4500 / 5000: loss 2.063358\n",
      "iteration 4600 / 5000: loss 2.098196\n",
      "iteration 4700 / 5000: loss 2.077927\n",
      "iteration 4800 / 5000: loss 2.083581\n",
      "iteration 4900 / 5000: loss 2.047903\n",
      "(4e-08, 26000.0)\n",
      "iteration 0 / 5000: loss 797.310351\n",
      "iteration 100 / 5000: loss 525.211209\n",
      "iteration 200 / 5000: loss 346.748520\n",
      "iteration 300 / 5000: loss 229.173196\n",
      "iteration 400 / 5000: loss 151.544833\n",
      "iteration 500 / 5000: loss 100.490415\n",
      "iteration 600 / 5000: loss 66.984560\n",
      "iteration 700 / 5000: loss 44.790809\n",
      "iteration 800 / 5000: loss 30.306022\n",
      "iteration 900 / 5000: loss 20.640090\n",
      "iteration 1000 / 5000: loss 14.346027\n",
      "iteration 1100 / 5000: loss 10.162238\n",
      "iteration 1200 / 5000: loss 7.450410\n",
      "iteration 1300 / 5000: loss 5.549185\n",
      "iteration 1400 / 5000: loss 4.369202\n",
      "iteration 1500 / 5000: loss 3.583060\n",
      "iteration 1600 / 5000: loss 3.123887\n",
      "iteration 1700 / 5000: loss 2.754124\n",
      "iteration 1800 / 5000: loss 2.527628\n",
      "iteration 1900 / 5000: loss 2.310343\n",
      "iteration 2000 / 5000: loss 2.265887\n",
      "iteration 2100 / 5000: loss 2.248497\n",
      "iteration 2200 / 5000: loss 2.142160\n",
      "iteration 2300 / 5000: loss 2.145096\n",
      "iteration 2400 / 5000: loss 2.168578\n",
      "iteration 2500 / 5000: loss 2.155416\n",
      "iteration 2600 / 5000: loss 2.095634\n",
      "iteration 2700 / 5000: loss 2.117583\n",
      "iteration 2800 / 5000: loss 2.144190\n",
      "iteration 2900 / 5000: loss 2.111857\n",
      "iteration 3000 / 5000: loss 2.133745\n",
      "iteration 3100 / 5000: loss 2.086391\n",
      "iteration 3200 / 5000: loss 2.140456\n",
      "iteration 3300 / 5000: loss 2.118661\n",
      "iteration 3400 / 5000: loss 2.103020\n",
      "iteration 3500 / 5000: loss 2.142830\n",
      "iteration 3600 / 5000: loss 2.098173\n",
      "iteration 3700 / 5000: loss 2.052747\n",
      "iteration 3800 / 5000: loss 2.091185\n",
      "iteration 3900 / 5000: loss 2.091160\n",
      "iteration 4000 / 5000: loss 2.102905\n",
      "iteration 4100 / 5000: loss 2.097162\n",
      "iteration 4200 / 5000: loss 2.094825\n",
      "iteration 4300 / 5000: loss 2.038272\n",
      "iteration 4400 / 5000: loss 2.086951\n",
      "iteration 4500 / 5000: loss 2.050924\n",
      "iteration 4600 / 5000: loss 2.112110\n",
      "iteration 4700 / 5000: loss 2.076129\n",
      "iteration 4800 / 5000: loss 2.072847\n",
      "iteration 4900 / 5000: loss 2.103513\n",
      "(5e-08, 16000.0)\n",
      "iteration 0 / 5000: loss 489.750891\n",
      "iteration 100 / 5000: loss 354.878341\n",
      "iteration 200 / 5000: loss 257.287878\n",
      "iteration 300 / 5000: loss 187.147523\n",
      "iteration 400 / 5000: loss 136.205085\n",
      "iteration 500 / 5000: loss 99.262352\n",
      "iteration 600 / 5000: loss 72.532577\n",
      "iteration 700 / 5000: loss 53.121052\n",
      "iteration 800 / 5000: loss 39.190047\n",
      "iteration 900 / 5000: loss 28.919948\n",
      "iteration 1000 / 5000: loss 21.487355\n",
      "iteration 1100 / 5000: loss 16.189885\n",
      "iteration 1200 / 5000: loss 12.348517\n",
      "iteration 1300 / 5000: loss 9.503574\n",
      "iteration 1400 / 5000: loss 7.406501\n",
      "iteration 1500 / 5000: loss 5.954355\n",
      "iteration 1600 / 5000: loss 4.900695\n",
      "iteration 1700 / 5000: loss 4.117247\n",
      "iteration 1800 / 5000: loss 3.577406\n",
      "iteration 1900 / 5000: loss 3.084771\n",
      "iteration 2000 / 5000: loss 2.817701\n",
      "iteration 2100 / 5000: loss 2.578073\n",
      "iteration 2200 / 5000: loss 2.457294\n",
      "iteration 2300 / 5000: loss 2.315300\n",
      "iteration 2400 / 5000: loss 2.296252\n",
      "iteration 2500 / 5000: loss 2.203692\n",
      "iteration 2600 / 5000: loss 2.159782\n",
      "iteration 2700 / 5000: loss 2.147126\n",
      "iteration 2800 / 5000: loss 2.041957\n",
      "iteration 2900 / 5000: loss 2.114736\n",
      "iteration 3000 / 5000: loss 2.063834\n",
      "iteration 3100 / 5000: loss 2.109700\n",
      "iteration 3200 / 5000: loss 2.055335\n",
      "iteration 3300 / 5000: loss 2.076863\n",
      "iteration 3400 / 5000: loss 2.072054\n",
      "iteration 3500 / 5000: loss 2.076283\n",
      "iteration 3600 / 5000: loss 2.061198\n",
      "iteration 3700 / 5000: loss 2.028856\n",
      "iteration 3800 / 5000: loss 2.028914\n",
      "iteration 3900 / 5000: loss 2.059526\n",
      "iteration 4000 / 5000: loss 2.021648\n",
      "iteration 4100 / 5000: loss 2.059937\n",
      "iteration 4200 / 5000: loss 2.054432\n",
      "iteration 4300 / 5000: loss 2.078216\n",
      "iteration 4400 / 5000: loss 2.000497\n",
      "iteration 4500 / 5000: loss 2.026301\n",
      "iteration 4600 / 5000: loss 2.078000\n",
      "iteration 4700 / 5000: loss 2.065609\n",
      "iteration 4800 / 5000: loss 2.074855\n",
      "iteration 4900 / 5000: loss 2.105680\n",
      "(5e-08, 18000.0)\n",
      "iteration 0 / 5000: loss 557.145357\n",
      "iteration 100 / 5000: loss 387.903817\n",
      "iteration 200 / 5000: loss 270.553960\n",
      "iteration 300 / 5000: loss 188.857811\n",
      "iteration 400 / 5000: loss 132.165666\n",
      "iteration 500 / 5000: loss 92.626773\n",
      "iteration 600 / 5000: loss 65.226004\n",
      "iteration 700 / 5000: loss 46.120786\n",
      "iteration 800 / 5000: loss 32.616787\n",
      "iteration 900 / 5000: loss 23.431392\n",
      "iteration 1000 / 5000: loss 16.885027\n",
      "iteration 1100 / 5000: loss 12.418394\n",
      "iteration 1200 / 5000: loss 9.331314\n",
      "iteration 1300 / 5000: loss 7.135416\n",
      "iteration 1400 / 5000: loss 5.519510\n",
      "iteration 1500 / 5000: loss 4.561657\n",
      "iteration 1600 / 5000: loss 3.735658\n",
      "iteration 1700 / 5000: loss 3.119524\n",
      "iteration 1800 / 5000: loss 2.853244\n",
      "iteration 1900 / 5000: loss 2.626503\n",
      "iteration 2000 / 5000: loss 2.467704\n",
      "iteration 2100 / 5000: loss 2.291857\n",
      "iteration 2200 / 5000: loss 2.240628\n",
      "iteration 2300 / 5000: loss 2.155076\n",
      "iteration 2400 / 5000: loss 2.159865\n",
      "iteration 2500 / 5000: loss 2.179440\n",
      "iteration 2600 / 5000: loss 2.105435\n",
      "iteration 2700 / 5000: loss 2.061444\n",
      "iteration 2800 / 5000: loss 2.001020\n",
      "iteration 2900 / 5000: loss 2.028224\n",
      "iteration 3000 / 5000: loss 2.062068\n",
      "iteration 3100 / 5000: loss 2.034409\n",
      "iteration 3200 / 5000: loss 2.025111\n",
      "iteration 3300 / 5000: loss 2.071240\n",
      "iteration 3400 / 5000: loss 2.065593\n",
      "iteration 3500 / 5000: loss 2.048947\n",
      "iteration 3600 / 5000: loss 2.052153\n",
      "iteration 3700 / 5000: loss 2.040522\n",
      "iteration 3800 / 5000: loss 2.036659\n",
      "iteration 3900 / 5000: loss 2.077601\n",
      "iteration 4000 / 5000: loss 2.098167\n",
      "iteration 4100 / 5000: loss 2.070099\n",
      "iteration 4200 / 5000: loss 2.054647\n",
      "iteration 4300 / 5000: loss 2.070229\n",
      "iteration 4400 / 5000: loss 2.064842\n",
      "iteration 4500 / 5000: loss 1.965386\n",
      "iteration 4600 / 5000: loss 2.045957\n",
      "iteration 4700 / 5000: loss 2.017691\n",
      "iteration 4800 / 5000: loss 2.102215\n",
      "iteration 4900 / 5000: loss 2.111598\n",
      "(5e-08, 20000.0)\n",
      "iteration 0 / 5000: loss 617.859744\n",
      "iteration 100 / 5000: loss 412.903566\n",
      "iteration 200 / 5000: loss 277.111217\n",
      "iteration 300 / 5000: loss 186.090433\n",
      "iteration 400 / 5000: loss 125.002889\n",
      "iteration 500 / 5000: loss 84.449197\n",
      "iteration 600 / 5000: loss 57.151962\n",
      "iteration 700 / 5000: loss 38.878725\n",
      "iteration 800 / 5000: loss 26.790525\n",
      "iteration 900 / 5000: loss 18.539002\n",
      "iteration 1000 / 5000: loss 13.136626\n",
      "iteration 1100 / 5000: loss 9.430920\n",
      "iteration 1200 / 5000: loss 7.014821\n",
      "iteration 1300 / 5000: loss 5.354850\n",
      "iteration 1400 / 5000: loss 4.270842\n",
      "iteration 1500 / 5000: loss 3.542072\n",
      "iteration 1600 / 5000: loss 3.065919\n",
      "iteration 1700 / 5000: loss 2.724692\n",
      "iteration 1800 / 5000: loss 2.492436\n",
      "iteration 1900 / 5000: loss 2.397002\n",
      "iteration 2000 / 5000: loss 2.267700\n",
      "iteration 2100 / 5000: loss 2.160691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2200 / 5000: loss 2.106345\n",
      "iteration 2300 / 5000: loss 2.061094\n",
      "iteration 2400 / 5000: loss 2.079584\n",
      "iteration 2500 / 5000: loss 2.096828\n",
      "iteration 2600 / 5000: loss 2.093480\n",
      "iteration 2700 / 5000: loss 2.049613\n",
      "iteration 2800 / 5000: loss 2.102242\n",
      "iteration 2900 / 5000: loss 2.107299\n",
      "iteration 3000 / 5000: loss 2.154685\n",
      "iteration 3100 / 5000: loss 2.095497\n",
      "iteration 3200 / 5000: loss 2.099074\n",
      "iteration 3300 / 5000: loss 2.005981\n",
      "iteration 3400 / 5000: loss 2.122298\n",
      "iteration 3500 / 5000: loss 2.063860\n",
      "iteration 3600 / 5000: loss 2.094940\n",
      "iteration 3700 / 5000: loss 2.058245\n",
      "iteration 3800 / 5000: loss 2.088296\n",
      "iteration 3900 / 5000: loss 2.107952\n",
      "iteration 4000 / 5000: loss 2.039658\n",
      "iteration 4100 / 5000: loss 2.031079\n",
      "iteration 4200 / 5000: loss 2.090162\n",
      "iteration 4300 / 5000: loss 2.128187\n",
      "iteration 4400 / 5000: loss 2.019701\n",
      "iteration 4500 / 5000: loss 2.081428\n",
      "iteration 4600 / 5000: loss 2.059457\n",
      "iteration 4700 / 5000: loss 2.064617\n",
      "iteration 4800 / 5000: loss 2.035927\n",
      "iteration 4900 / 5000: loss 2.032496\n",
      "(5e-08, 23000.0)\n",
      "iteration 0 / 5000: loss 707.167670\n",
      "iteration 100 / 5000: loss 446.244465\n",
      "iteration 200 / 5000: loss 281.603935\n",
      "iteration 300 / 5000: loss 178.317255\n",
      "iteration 400 / 5000: loss 113.034520\n",
      "iteration 500 / 5000: loss 72.018189\n",
      "iteration 600 / 5000: loss 46.122583\n",
      "iteration 700 / 5000: loss 29.924880\n",
      "iteration 800 / 5000: loss 19.582653\n",
      "iteration 900 / 5000: loss 13.114149\n",
      "iteration 1000 / 5000: loss 8.968558\n",
      "iteration 1100 / 5000: loss 6.472436\n",
      "iteration 1200 / 5000: loss 4.790016\n",
      "iteration 1300 / 5000: loss 3.845570\n",
      "iteration 1400 / 5000: loss 3.213623\n",
      "iteration 1500 / 5000: loss 2.755969\n",
      "iteration 1600 / 5000: loss 2.449399\n",
      "iteration 1700 / 5000: loss 2.344228\n",
      "iteration 1800 / 5000: loss 2.280031\n",
      "iteration 1900 / 5000: loss 2.158697\n",
      "iteration 2000 / 5000: loss 2.105076\n",
      "iteration 2100 / 5000: loss 2.132805\n",
      "iteration 2200 / 5000: loss 2.058856\n",
      "iteration 2300 / 5000: loss 2.079732\n",
      "iteration 2400 / 5000: loss 2.126765\n",
      "iteration 2500 / 5000: loss 2.066988\n",
      "iteration 2600 / 5000: loss 2.032919\n",
      "iteration 2700 / 5000: loss 2.097468\n",
      "iteration 2800 / 5000: loss 2.067639\n",
      "iteration 2900 / 5000: loss 2.044034\n",
      "iteration 3000 / 5000: loss 2.056526\n",
      "iteration 3100 / 5000: loss 2.094579\n",
      "iteration 3200 / 5000: loss 2.048427\n",
      "iteration 3300 / 5000: loss 2.099380\n",
      "iteration 3400 / 5000: loss 2.087640\n",
      "iteration 3500 / 5000: loss 2.024920\n",
      "iteration 3600 / 5000: loss 2.048789\n",
      "iteration 3700 / 5000: loss 2.067998\n",
      "iteration 3800 / 5000: loss 2.048992\n",
      "iteration 3900 / 5000: loss 2.148309\n",
      "iteration 4000 / 5000: loss 2.113659\n",
      "iteration 4100 / 5000: loss 2.043599\n",
      "iteration 4200 / 5000: loss 2.091823\n",
      "iteration 4300 / 5000: loss 2.064770\n",
      "iteration 4400 / 5000: loss 1.991487\n",
      "iteration 4500 / 5000: loss 2.003105\n",
      "iteration 4600 / 5000: loss 2.077003\n",
      "iteration 4700 / 5000: loss 2.083667\n",
      "iteration 4800 / 5000: loss 2.140110\n",
      "iteration 4900 / 5000: loss 2.050318\n",
      "(5e-08, 26000.0)\n",
      "iteration 0 / 5000: loss 799.673654\n",
      "iteration 100 / 5000: loss 474.675366\n",
      "iteration 200 / 5000: loss 282.311094\n",
      "iteration 300 / 5000: loss 168.347320\n",
      "iteration 400 / 5000: loss 100.709303\n",
      "iteration 500 / 5000: loss 60.593120\n",
      "iteration 600 / 5000: loss 36.704092\n",
      "iteration 700 / 5000: loss 22.770991\n",
      "iteration 800 / 5000: loss 14.327014\n",
      "iteration 900 / 5000: loss 9.342022\n",
      "iteration 1000 / 5000: loss 6.425350\n",
      "iteration 1100 / 5000: loss 4.610090\n",
      "iteration 1200 / 5000: loss 3.648073\n",
      "iteration 1300 / 5000: loss 3.046760\n",
      "iteration 1400 / 5000: loss 2.557818\n",
      "iteration 1500 / 5000: loss 2.376420\n",
      "iteration 1600 / 5000: loss 2.209611\n",
      "iteration 1700 / 5000: loss 2.219718\n",
      "iteration 1800 / 5000: loss 2.205564\n",
      "iteration 1900 / 5000: loss 2.136481\n",
      "iteration 2000 / 5000: loss 2.103982\n",
      "iteration 2100 / 5000: loss 2.142516\n",
      "iteration 2200 / 5000: loss 2.096474\n",
      "iteration 2300 / 5000: loss 2.066093\n",
      "iteration 2400 / 5000: loss 2.106360\n",
      "iteration 2500 / 5000: loss 2.086468\n",
      "iteration 2600 / 5000: loss 2.021179\n",
      "iteration 2700 / 5000: loss 2.110305\n",
      "iteration 2800 / 5000: loss 2.130815\n",
      "iteration 2900 / 5000: loss 2.069728\n",
      "iteration 3000 / 5000: loss 2.070442\n",
      "iteration 3100 / 5000: loss 2.079543\n",
      "iteration 3200 / 5000: loss 2.122086\n",
      "iteration 3300 / 5000: loss 2.098608\n",
      "iteration 3400 / 5000: loss 2.086541\n",
      "iteration 3500 / 5000: loss 2.063969\n",
      "iteration 3600 / 5000: loss 2.059579\n",
      "iteration 3700 / 5000: loss 2.026381\n",
      "iteration 3800 / 5000: loss 2.105039\n",
      "iteration 3900 / 5000: loss 2.060530\n",
      "iteration 4000 / 5000: loss 2.134524\n",
      "iteration 4100 / 5000: loss 2.079897\n",
      "iteration 4200 / 5000: loss 2.101643\n",
      "iteration 4300 / 5000: loss 2.155895\n",
      "iteration 4400 / 5000: loss 2.048824\n",
      "iteration 4500 / 5000: loss 2.044425\n",
      "iteration 4600 / 5000: loss 2.082010\n",
      "iteration 4700 / 5000: loss 2.087311\n",
      "iteration 4800 / 5000: loss 2.087764\n",
      "iteration 4900 / 5000: loss 2.110984\n",
      "(6e-08, 16000.0)\n",
      "iteration 0 / 5000: loss 492.438991\n",
      "iteration 100 / 5000: loss 335.439704\n",
      "iteration 200 / 5000: loss 228.424642\n",
      "iteration 300 / 5000: loss 155.858004\n",
      "iteration 400 / 5000: loss 106.686485\n",
      "iteration 500 / 5000: loss 73.186990\n",
      "iteration 600 / 5000: loss 50.405902\n",
      "iteration 700 / 5000: loss 34.924850\n",
      "iteration 800 / 5000: loss 24.481414\n",
      "iteration 900 / 5000: loss 17.223756\n",
      "iteration 1000 / 5000: loss 12.408888\n",
      "iteration 1100 / 5000: loss 9.037024\n",
      "iteration 1200 / 5000: loss 6.771714\n",
      "iteration 1300 / 5000: loss 5.323210\n",
      "iteration 1400 / 5000: loss 4.292169\n",
      "iteration 1500 / 5000: loss 3.543148\n",
      "iteration 1600 / 5000: loss 3.094559\n",
      "iteration 1700 / 5000: loss 2.764171\n",
      "iteration 1800 / 5000: loss 2.481406\n",
      "iteration 1900 / 5000: loss 2.402407\n",
      "iteration 2000 / 5000: loss 2.253167\n",
      "iteration 2100 / 5000: loss 2.221853\n",
      "iteration 2200 / 5000: loss 2.194727\n",
      "iteration 2300 / 5000: loss 2.056798\n",
      "iteration 2400 / 5000: loss 2.111711\n",
      "iteration 2500 / 5000: loss 2.141447\n",
      "iteration 2600 / 5000: loss 2.087393\n",
      "iteration 2700 / 5000: loss 2.064537\n",
      "iteration 2800 / 5000: loss 2.007070\n",
      "iteration 2900 / 5000: loss 2.042180\n",
      "iteration 3000 / 5000: loss 2.039353\n",
      "iteration 3100 / 5000: loss 2.081351\n",
      "iteration 3200 / 5000: loss 2.040427\n",
      "iteration 3300 / 5000: loss 2.062257\n",
      "iteration 3400 / 5000: loss 2.098079\n",
      "iteration 3500 / 5000: loss 2.096527\n",
      "iteration 3600 / 5000: loss 2.021870\n",
      "iteration 3700 / 5000: loss 2.019903\n",
      "iteration 3800 / 5000: loss 2.041977\n",
      "iteration 3900 / 5000: loss 2.135189\n",
      "iteration 4000 / 5000: loss 2.028093\n",
      "iteration 4100 / 5000: loss 2.029250\n",
      "iteration 4200 / 5000: loss 2.063995\n",
      "iteration 4300 / 5000: loss 2.055845\n",
      "iteration 4400 / 5000: loss 2.095641\n",
      "iteration 4500 / 5000: loss 2.015436\n",
      "iteration 4600 / 5000: loss 1.980650\n",
      "iteration 4700 / 5000: loss 2.031011\n",
      "iteration 4800 / 5000: loss 2.044754\n",
      "iteration 4900 / 5000: loss 2.072610\n",
      "(6e-08, 18000.0)\n",
      "iteration 0 / 5000: loss 559.937005\n",
      "iteration 100 / 5000: loss 362.962522\n",
      "iteration 200 / 5000: loss 235.817131\n",
      "iteration 300 / 5000: loss 153.427574\n",
      "iteration 400 / 5000: loss 100.116143\n",
      "iteration 500 / 5000: loss 65.622471\n",
      "iteration 600 / 5000: loss 43.309503\n",
      "iteration 700 / 5000: loss 28.786477\n",
      "iteration 800 / 5000: loss 19.397037\n",
      "iteration 900 / 5000: loss 13.249232\n",
      "iteration 1000 / 5000: loss 9.259360\n",
      "iteration 1100 / 5000: loss 6.757149\n",
      "iteration 1200 / 5000: loss 5.141733\n",
      "iteration 1300 / 5000: loss 4.004594\n",
      "iteration 1400 / 5000: loss 3.274934\n",
      "iteration 1500 / 5000: loss 2.929989\n",
      "iteration 1600 / 5000: loss 2.603763\n",
      "iteration 1700 / 5000: loss 2.415671\n",
      "iteration 1800 / 5000: loss 2.315772\n",
      "iteration 1900 / 5000: loss 2.207891\n",
      "iteration 2000 / 5000: loss 2.170944\n",
      "iteration 2100 / 5000: loss 2.089092\n",
      "iteration 2200 / 5000: loss 2.106551\n",
      "iteration 2300 / 5000: loss 2.086635\n",
      "iteration 2400 / 5000: loss 2.031883\n",
      "iteration 2500 / 5000: loss 2.076987\n",
      "iteration 2600 / 5000: loss 2.055256\n",
      "iteration 2700 / 5000: loss 2.065245\n",
      "iteration 2800 / 5000: loss 2.034027\n",
      "iteration 2900 / 5000: loss 2.038444\n",
      "iteration 3000 / 5000: loss 1.993837\n",
      "iteration 3100 / 5000: loss 2.012747\n",
      "iteration 3200 / 5000: loss 2.027651\n",
      "iteration 3300 / 5000: loss 2.072632\n",
      "iteration 3400 / 5000: loss 2.031182\n",
      "iteration 3500 / 5000: loss 2.088955\n",
      "iteration 3600 / 5000: loss 2.033215\n",
      "iteration 3700 / 5000: loss 2.038624\n",
      "iteration 3800 / 5000: loss 2.043137\n",
      "iteration 3900 / 5000: loss 2.079996\n",
      "iteration 4000 / 5000: loss 2.031534\n",
      "iteration 4100 / 5000: loss 2.061338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4200 / 5000: loss 2.041971\n",
      "iteration 4300 / 5000: loss 2.072993\n",
      "iteration 4400 / 5000: loss 2.035205\n",
      "iteration 4500 / 5000: loss 2.044923\n",
      "iteration 4600 / 5000: loss 2.060644\n",
      "iteration 4700 / 5000: loss 2.064278\n",
      "iteration 4800 / 5000: loss 2.092863\n",
      "iteration 4900 / 5000: loss 2.071281\n",
      "(6e-08, 20000.0)\n",
      "iteration 0 / 5000: loss 622.953677\n",
      "iteration 100 / 5000: loss 384.565799\n",
      "iteration 200 / 5000: loss 238.027480\n",
      "iteration 300 / 5000: loss 147.682407\n",
      "iteration 400 / 5000: loss 91.983454\n",
      "iteration 500 / 5000: loss 57.608360\n",
      "iteration 600 / 5000: loss 36.325708\n",
      "iteration 700 / 5000: loss 23.251334\n",
      "iteration 800 / 5000: loss 15.195434\n",
      "iteration 900 / 5000: loss 10.162087\n",
      "iteration 1000 / 5000: loss 7.100278\n",
      "iteration 1100 / 5000: loss 5.177339\n",
      "iteration 1200 / 5000: loss 3.919187\n",
      "iteration 1300 / 5000: loss 3.263064\n",
      "iteration 1400 / 5000: loss 2.757590\n",
      "iteration 1500 / 5000: loss 2.512370\n",
      "iteration 1600 / 5000: loss 2.326269\n",
      "iteration 1700 / 5000: loss 2.224740\n",
      "iteration 1800 / 5000: loss 2.196140\n",
      "iteration 1900 / 5000: loss 2.107944\n",
      "iteration 2000 / 5000: loss 2.077215\n",
      "iteration 2100 / 5000: loss 2.071940\n",
      "iteration 2200 / 5000: loss 2.058294\n",
      "iteration 2300 / 5000: loss 2.148600\n",
      "iteration 2400 / 5000: loss 2.037355\n",
      "iteration 2500 / 5000: loss 2.046261\n",
      "iteration 2600 / 5000: loss 2.104273\n",
      "iteration 2700 / 5000: loss 2.102856\n",
      "iteration 2800 / 5000: loss 2.146249\n",
      "iteration 2900 / 5000: loss 2.118419\n",
      "iteration 3000 / 5000: loss 2.073419\n",
      "iteration 3100 / 5000: loss 2.112758\n",
      "iteration 3200 / 5000: loss 2.020782\n",
      "iteration 3300 / 5000: loss 2.115833\n",
      "iteration 3400 / 5000: loss 2.057488\n",
      "iteration 3500 / 5000: loss 2.005857\n",
      "iteration 3600 / 5000: loss 2.068620\n",
      "iteration 3700 / 5000: loss 2.123523\n",
      "iteration 3800 / 5000: loss 2.037701\n",
      "iteration 3900 / 5000: loss 2.098129\n",
      "iteration 4000 / 5000: loss 2.042444\n",
      "iteration 4100 / 5000: loss 2.097539\n",
      "iteration 4200 / 5000: loss 2.018377\n",
      "iteration 4300 / 5000: loss 2.009516\n",
      "iteration 4400 / 5000: loss 2.053492\n",
      "iteration 4500 / 5000: loss 2.098948\n",
      "iteration 4600 / 5000: loss 2.090078\n",
      "iteration 4700 / 5000: loss 2.124727\n",
      "iteration 4800 / 5000: loss 2.068122\n",
      "iteration 4900 / 5000: loss 2.076878\n",
      "(6e-08, 23000.0)\n",
      "iteration 0 / 5000: loss 718.661765\n",
      "iteration 100 / 5000: loss 412.508537\n",
      "iteration 200 / 5000: loss 237.529502\n",
      "iteration 300 / 5000: loss 137.244492\n",
      "iteration 400 / 5000: loss 79.668654\n",
      "iteration 500 / 5000: loss 46.653299\n",
      "iteration 600 / 5000: loss 27.734435\n",
      "iteration 700 / 5000: loss 16.815187\n",
      "iteration 800 / 5000: loss 10.490066\n",
      "iteration 900 / 5000: loss 6.950603\n",
      "iteration 1000 / 5000: loss 4.899908\n",
      "iteration 1100 / 5000: loss 3.669892\n",
      "iteration 1200 / 5000: loss 2.978089\n",
      "iteration 1300 / 5000: loss 2.574908\n",
      "iteration 1400 / 5000: loss 2.387511\n",
      "iteration 1500 / 5000: loss 2.281106\n",
      "iteration 1600 / 5000: loss 2.119020\n",
      "iteration 1700 / 5000: loss 2.115037\n",
      "iteration 1800 / 5000: loss 2.057426\n",
      "iteration 1900 / 5000: loss 2.061504\n",
      "iteration 2000 / 5000: loss 2.059741\n",
      "iteration 2100 / 5000: loss 2.051335\n",
      "iteration 2200 / 5000: loss 2.040324\n",
      "iteration 2300 / 5000: loss 2.080097\n",
      "iteration 2400 / 5000: loss 2.044339\n",
      "iteration 2500 / 5000: loss 2.056260\n",
      "iteration 2600 / 5000: loss 1.997111\n",
      "iteration 2700 / 5000: loss 2.029851\n",
      "iteration 2800 / 5000: loss 2.115355\n",
      "iteration 2900 / 5000: loss 2.113281\n",
      "iteration 3000 / 5000: loss 2.100420\n",
      "iteration 3100 / 5000: loss 2.078013\n",
      "iteration 3200 / 5000: loss 2.064553\n",
      "iteration 3300 / 5000: loss 2.094761\n",
      "iteration 3400 / 5000: loss 2.127386\n",
      "iteration 3500 / 5000: loss 2.089822\n",
      "iteration 3600 / 5000: loss 2.035372\n",
      "iteration 3700 / 5000: loss 2.091675\n",
      "iteration 3800 / 5000: loss 2.064716\n",
      "iteration 3900 / 5000: loss 2.069460\n",
      "iteration 4000 / 5000: loss 2.091247\n",
      "iteration 4100 / 5000: loss 2.036635\n",
      "iteration 4200 / 5000: loss 2.005587\n",
      "iteration 4300 / 5000: loss 2.090704\n",
      "iteration 4400 / 5000: loss 2.083348\n",
      "iteration 4500 / 5000: loss 2.122598\n",
      "iteration 4600 / 5000: loss 2.046587\n",
      "iteration 4700 / 5000: loss 2.067003\n",
      "iteration 4800 / 5000: loss 2.042184\n",
      "iteration 4900 / 5000: loss 2.066421\n",
      "(6e-08, 26000.0)\n",
      "iteration 0 / 5000: loss 798.992323\n",
      "iteration 100 / 5000: loss 427.656549\n",
      "iteration 200 / 5000: loss 229.535068\n",
      "iteration 300 / 5000: loss 123.631886\n",
      "iteration 400 / 5000: loss 67.108340\n",
      "iteration 500 / 5000: loss 36.846084\n",
      "iteration 600 / 5000: loss 20.615428\n",
      "iteration 700 / 5000: loss 12.035951\n",
      "iteration 800 / 5000: loss 7.309030\n",
      "iteration 900 / 5000: loss 4.917001\n",
      "iteration 1000 / 5000: loss 3.601327\n",
      "iteration 1100 / 5000: loss 2.890290\n",
      "iteration 1200 / 5000: loss 2.460108\n",
      "iteration 1300 / 5000: loss 2.371595\n",
      "iteration 1400 / 5000: loss 2.190442\n",
      "iteration 1500 / 5000: loss 2.148026\n",
      "iteration 1600 / 5000: loss 2.106294\n",
      "iteration 1700 / 5000: loss 2.114731\n",
      "iteration 1800 / 5000: loss 2.094617\n",
      "iteration 1900 / 5000: loss 2.068124\n",
      "iteration 2000 / 5000: loss 2.100945\n",
      "iteration 2100 / 5000: loss 2.045570\n",
      "iteration 2200 / 5000: loss 2.110408\n",
      "iteration 2300 / 5000: loss 2.127688\n",
      "iteration 2400 / 5000: loss 2.094342\n",
      "iteration 2500 / 5000: loss 2.091442\n",
      "iteration 2600 / 5000: loss 2.057937\n",
      "iteration 2700 / 5000: loss 2.077708\n",
      "iteration 2800 / 5000: loss 2.117746\n",
      "iteration 2900 / 5000: loss 2.110506\n",
      "iteration 3000 / 5000: loss 2.116986\n",
      "iteration 3100 / 5000: loss 2.098396\n",
      "iteration 3200 / 5000: loss 2.037489\n",
      "iteration 3300 / 5000: loss 2.077330\n",
      "iteration 3400 / 5000: loss 2.049410\n",
      "iteration 3500 / 5000: loss 2.180273\n",
      "iteration 3600 / 5000: loss 2.069922\n",
      "iteration 3700 / 5000: loss 2.088587\n",
      "iteration 3800 / 5000: loss 2.099092\n",
      "iteration 3900 / 5000: loss 2.098519\n",
      "iteration 4000 / 5000: loss 2.110256\n",
      "iteration 4100 / 5000: loss 2.072023\n",
      "iteration 4200 / 5000: loss 2.115858\n",
      "iteration 4300 / 5000: loss 2.094222\n",
      "iteration 4400 / 5000: loss 2.060499\n",
      "iteration 4500 / 5000: loss 2.084637\n",
      "iteration 4600 / 5000: loss 2.099094\n",
      "iteration 4700 / 5000: loss 2.131261\n",
      "iteration 4800 / 5000: loss 2.058763\n",
      "iteration 4900 / 5000: loss 2.116572\n",
      "(7e-08, 16000.0)\n",
      "iteration 0 / 5000: loss 497.801026\n",
      "iteration 100 / 5000: loss 317.242161\n",
      "iteration 200 / 5000: loss 202.973298\n",
      "iteration 300 / 5000: loss 129.998712\n",
      "iteration 400 / 5000: loss 83.521585\n",
      "iteration 500 / 5000: loss 54.100024\n",
      "iteration 600 / 5000: loss 35.171675\n",
      "iteration 700 / 5000: loss 23.130716\n",
      "iteration 800 / 5000: loss 15.577578\n",
      "iteration 900 / 5000: loss 10.617294\n",
      "iteration 1000 / 5000: loss 7.539986\n",
      "iteration 1100 / 5000: loss 5.491987\n",
      "iteration 1200 / 5000: loss 4.221283\n",
      "iteration 1300 / 5000: loss 3.400284\n",
      "iteration 1400 / 5000: loss 2.958281\n",
      "iteration 1500 / 5000: loss 2.639309\n",
      "iteration 1600 / 5000: loss 2.380380\n",
      "iteration 1700 / 5000: loss 2.319576\n",
      "iteration 1800 / 5000: loss 2.205396\n",
      "iteration 1900 / 5000: loss 2.103711\n",
      "iteration 2000 / 5000: loss 2.114927\n",
      "iteration 2100 / 5000: loss 2.089242\n",
      "iteration 2200 / 5000: loss 2.115295\n",
      "iteration 2300 / 5000: loss 2.031960\n",
      "iteration 2400 / 5000: loss 2.010650\n",
      "iteration 2500 / 5000: loss 2.104869\n",
      "iteration 2600 / 5000: loss 2.029567\n",
      "iteration 2700 / 5000: loss 2.059923\n",
      "iteration 2800 / 5000: loss 2.020522\n",
      "iteration 2900 / 5000: loss 1.995393\n",
      "iteration 3000 / 5000: loss 2.072269\n",
      "iteration 3100 / 5000: loss 2.002229\n",
      "iteration 3200 / 5000: loss 2.008026\n",
      "iteration 3300 / 5000: loss 2.056610\n",
      "iteration 3400 / 5000: loss 2.097043\n",
      "iteration 3500 / 5000: loss 2.123410\n",
      "iteration 3600 / 5000: loss 2.064463\n",
      "iteration 3700 / 5000: loss 2.044585\n",
      "iteration 3800 / 5000: loss 2.070029\n",
      "iteration 3900 / 5000: loss 2.014353\n",
      "iteration 4000 / 5000: loss 2.021053\n",
      "iteration 4100 / 5000: loss 2.048762\n",
      "iteration 4200 / 5000: loss 2.024033\n",
      "iteration 4300 / 5000: loss 2.087557\n",
      "iteration 4400 / 5000: loss 2.036873\n",
      "iteration 4500 / 5000: loss 2.027346\n",
      "iteration 4600 / 5000: loss 2.026996\n",
      "iteration 4700 / 5000: loss 2.062750\n",
      "iteration 4800 / 5000: loss 2.062625\n",
      "iteration 4900 / 5000: loss 2.053548\n",
      "(7e-08, 18000.0)\n",
      "iteration 0 / 5000: loss 559.237013\n",
      "iteration 100 / 5000: loss 336.892673\n",
      "iteration 200 / 5000: loss 203.850354\n",
      "iteration 300 / 5000: loss 123.696581\n",
      "iteration 400 / 5000: loss 75.342914\n",
      "iteration 500 / 5000: loss 46.154235\n",
      "iteration 600 / 5000: loss 28.713738\n",
      "iteration 700 / 5000: loss 18.036636\n",
      "iteration 800 / 5000: loss 11.736880\n",
      "iteration 900 / 5000: loss 7.881507\n",
      "iteration 1000 / 5000: loss 5.555312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1100 / 5000: loss 4.173565\n",
      "iteration 1200 / 5000: loss 3.385446\n",
      "iteration 1300 / 5000: loss 2.895139\n",
      "iteration 1400 / 5000: loss 2.539366\n",
      "iteration 1500 / 5000: loss 2.320576\n",
      "iteration 1600 / 5000: loss 2.258358\n",
      "iteration 1700 / 5000: loss 2.141827\n",
      "iteration 1800 / 5000: loss 2.099589\n",
      "iteration 1900 / 5000: loss 2.104411\n",
      "iteration 2000 / 5000: loss 2.077946\n",
      "iteration 2100 / 5000: loss 2.047843\n",
      "iteration 2200 / 5000: loss 2.106149\n",
      "iteration 2300 / 5000: loss 1.991909\n",
      "iteration 2400 / 5000: loss 2.079659\n",
      "iteration 2500 / 5000: loss 2.039143\n",
      "iteration 2600 / 5000: loss 2.088116\n",
      "iteration 2700 / 5000: loss 2.109471\n",
      "iteration 2800 / 5000: loss 2.047911\n",
      "iteration 2900 / 5000: loss 2.093352\n",
      "iteration 3000 / 5000: loss 2.000370\n",
      "iteration 3100 / 5000: loss 2.071080\n",
      "iteration 3200 / 5000: loss 2.044172\n",
      "iteration 3300 / 5000: loss 2.025646\n",
      "iteration 3400 / 5000: loss 2.050660\n",
      "iteration 3500 / 5000: loss 1.975560\n",
      "iteration 3600 / 5000: loss 2.095233\n",
      "iteration 3700 / 5000: loss 2.015020\n",
      "iteration 3800 / 5000: loss 2.095859\n",
      "iteration 3900 / 5000: loss 2.080241\n",
      "iteration 4000 / 5000: loss 2.043340\n",
      "iteration 4100 / 5000: loss 2.017773\n",
      "iteration 4200 / 5000: loss 2.024855\n",
      "iteration 4300 / 5000: loss 2.067025\n",
      "iteration 4400 / 5000: loss 2.029782\n",
      "iteration 4500 / 5000: loss 2.030527\n",
      "iteration 4600 / 5000: loss 2.117225\n",
      "iteration 4700 / 5000: loss 2.064094\n",
      "iteration 4800 / 5000: loss 1.994689\n",
      "iteration 4900 / 5000: loss 2.038325\n",
      "(7e-08, 20000.0)\n",
      "iteration 0 / 5000: loss 622.429171\n",
      "iteration 100 / 5000: loss 355.474655\n",
      "iteration 200 / 5000: loss 203.270252\n",
      "iteration 300 / 5000: loss 116.564443\n",
      "iteration 400 / 5000: loss 67.349621\n",
      "iteration 500 / 5000: loss 39.345643\n",
      "iteration 600 / 5000: loss 23.311837\n",
      "iteration 700 / 5000: loss 14.142290\n",
      "iteration 800 / 5000: loss 8.949522\n",
      "iteration 900 / 5000: loss 5.964906\n",
      "iteration 1000 / 5000: loss 4.361674\n",
      "iteration 1100 / 5000: loss 3.274049\n",
      "iteration 1200 / 5000: loss 2.784083\n",
      "iteration 1300 / 5000: loss 2.492163\n",
      "iteration 1400 / 5000: loss 2.276727\n",
      "iteration 1500 / 5000: loss 2.187148\n",
      "iteration 1600 / 5000: loss 2.123066\n",
      "iteration 1700 / 5000: loss 2.125149\n",
      "iteration 1800 / 5000: loss 2.079406\n",
      "iteration 1900 / 5000: loss 2.056077\n",
      "iteration 2000 / 5000: loss 2.123935\n",
      "iteration 2100 / 5000: loss 2.084023\n",
      "iteration 2200 / 5000: loss 2.073083\n",
      "iteration 2300 / 5000: loss 2.095201\n",
      "iteration 2400 / 5000: loss 2.050656\n",
      "iteration 2500 / 5000: loss 2.045531\n",
      "iteration 2600 / 5000: loss 2.020894\n",
      "iteration 2700 / 5000: loss 2.027547\n",
      "iteration 2800 / 5000: loss 2.033336\n",
      "iteration 2900 / 5000: loss 2.098303\n",
      "iteration 3000 / 5000: loss 2.132724\n",
      "iteration 3100 / 5000: loss 2.096822\n",
      "iteration 3200 / 5000: loss 2.072878\n",
      "iteration 3300 / 5000: loss 2.065658\n",
      "iteration 3400 / 5000: loss 2.076713\n",
      "iteration 3500 / 5000: loss 2.128767\n",
      "iteration 3600 / 5000: loss 2.109675\n",
      "iteration 3700 / 5000: loss 2.029971\n",
      "iteration 3800 / 5000: loss 2.074851\n",
      "iteration 3900 / 5000: loss 2.056050\n",
      "iteration 4000 / 5000: loss 2.084648\n",
      "iteration 4100 / 5000: loss 2.051954\n",
      "iteration 4200 / 5000: loss 2.046916\n",
      "iteration 4300 / 5000: loss 2.047702\n",
      "iteration 4400 / 5000: loss 2.090660\n",
      "iteration 4500 / 5000: loss 2.024436\n",
      "iteration 4600 / 5000: loss 2.031030\n",
      "iteration 4700 / 5000: loss 2.100652\n",
      "iteration 4800 / 5000: loss 2.077957\n",
      "iteration 4900 / 5000: loss 2.053513\n",
      "(7e-08, 23000.0)\n",
      "iteration 0 / 5000: loss 707.838510\n",
      "iteration 100 / 5000: loss 371.013997\n",
      "iteration 200 / 5000: loss 195.152001\n",
      "iteration 300 / 5000: loss 103.194344\n",
      "iteration 400 / 5000: loss 55.032871\n",
      "iteration 500 / 5000: loss 29.801993\n",
      "iteration 600 / 5000: loss 16.621089\n",
      "iteration 700 / 5000: loss 9.643065\n",
      "iteration 800 / 5000: loss 6.034417\n",
      "iteration 900 / 5000: loss 4.136709\n",
      "iteration 1000 / 5000: loss 3.104290\n",
      "iteration 1100 / 5000: loss 2.709859\n",
      "iteration 1200 / 5000: loss 2.420241\n",
      "iteration 1300 / 5000: loss 2.231251\n",
      "iteration 1400 / 5000: loss 2.121185\n",
      "iteration 1500 / 5000: loss 2.113737\n",
      "iteration 1600 / 5000: loss 2.080443\n",
      "iteration 1700 / 5000: loss 2.079478\n",
      "iteration 1800 / 5000: loss 2.102464\n",
      "iteration 1900 / 5000: loss 2.026172\n",
      "iteration 2000 / 5000: loss 2.073947\n",
      "iteration 2100 / 5000: loss 2.006724\n",
      "iteration 2200 / 5000: loss 2.061318\n",
      "iteration 2300 / 5000: loss 2.116979\n",
      "iteration 2400 / 5000: loss 2.106930\n",
      "iteration 2500 / 5000: loss 2.079459\n",
      "iteration 2600 / 5000: loss 2.068284\n",
      "iteration 2700 / 5000: loss 2.087222\n",
      "iteration 2800 / 5000: loss 2.106001\n",
      "iteration 2900 / 5000: loss 2.022040\n",
      "iteration 3000 / 5000: loss 2.120544\n",
      "iteration 3100 / 5000: loss 2.101428\n",
      "iteration 3200 / 5000: loss 2.095159\n",
      "iteration 3300 / 5000: loss 2.041052\n",
      "iteration 3400 / 5000: loss 2.062522\n",
      "iteration 3500 / 5000: loss 2.039156\n",
      "iteration 3600 / 5000: loss 2.084562\n",
      "iteration 3700 / 5000: loss 2.055628\n",
      "iteration 3800 / 5000: loss 2.072852\n",
      "iteration 3900 / 5000: loss 2.148627\n",
      "iteration 4000 / 5000: loss 2.164013\n",
      "iteration 4100 / 5000: loss 2.128747\n",
      "iteration 4200 / 5000: loss 2.056070\n",
      "iteration 4300 / 5000: loss 2.059132\n",
      "iteration 4400 / 5000: loss 2.060389\n",
      "iteration 4500 / 5000: loss 2.126287\n",
      "iteration 4600 / 5000: loss 2.042435\n",
      "iteration 4700 / 5000: loss 2.022584\n",
      "iteration 4800 / 5000: loss 2.082396\n",
      "iteration 4900 / 5000: loss 2.105313\n",
      "(7e-08, 26000.0)\n",
      "iteration 0 / 5000: loss 816.884659\n",
      "iteration 100 / 5000: loss 393.846404\n",
      "iteration 200 / 5000: loss 190.261842\n",
      "iteration 300 / 5000: loss 92.670513\n",
      "iteration 400 / 5000: loss 45.663314\n",
      "iteration 500 / 5000: loss 23.111784\n",
      "iteration 600 / 5000: loss 12.179301\n",
      "iteration 700 / 5000: loss 6.888063\n",
      "iteration 800 / 5000: loss 4.421246\n",
      "iteration 900 / 5000: loss 3.217372\n",
      "iteration 1000 / 5000: loss 2.612279\n",
      "iteration 1100 / 5000: loss 2.328896\n",
      "iteration 1200 / 5000: loss 2.206987\n",
      "iteration 1300 / 5000: loss 2.182895\n",
      "iteration 1400 / 5000: loss 2.087133\n",
      "iteration 1500 / 5000: loss 2.057612\n",
      "iteration 1600 / 5000: loss 2.068993\n",
      "iteration 1700 / 5000: loss 2.103291\n",
      "iteration 1800 / 5000: loss 2.012232\n",
      "iteration 1900 / 5000: loss 2.076751\n",
      "iteration 2000 / 5000: loss 2.084074\n",
      "iteration 2100 / 5000: loss 2.097100\n",
      "iteration 2200 / 5000: loss 2.110690\n",
      "iteration 2300 / 5000: loss 2.142692\n",
      "iteration 2400 / 5000: loss 2.121473\n",
      "iteration 2500 / 5000: loss 2.047049\n",
      "iteration 2600 / 5000: loss 2.085494\n",
      "iteration 2700 / 5000: loss 2.097105\n",
      "iteration 2800 / 5000: loss 2.115650\n",
      "iteration 2900 / 5000: loss 2.085469\n",
      "iteration 3000 / 5000: loss 2.096707\n",
      "iteration 3100 / 5000: loss 2.078777\n",
      "iteration 3200 / 5000: loss 2.081466\n",
      "iteration 3300 / 5000: loss 2.094778\n",
      "iteration 3400 / 5000: loss 2.169910\n",
      "iteration 3500 / 5000: loss 2.073052\n",
      "iteration 3600 / 5000: loss 2.075456\n",
      "iteration 3700 / 5000: loss 2.138560\n",
      "iteration 3800 / 5000: loss 2.085759\n",
      "iteration 3900 / 5000: loss 2.120978\n",
      "iteration 4000 / 5000: loss 2.081700\n",
      "iteration 4100 / 5000: loss 2.058460\n",
      "iteration 4200 / 5000: loss 2.047194\n",
      "iteration 4300 / 5000: loss 2.069755\n",
      "iteration 4400 / 5000: loss 2.093902\n",
      "iteration 4500 / 5000: loss 2.126596\n",
      "iteration 4600 / 5000: loss 2.045185\n",
      "iteration 4700 / 5000: loss 2.049608\n",
      "iteration 4800 / 5000: loss 2.098940\n",
      "iteration 4900 / 5000: loss 2.104069\n",
      "(8e-08, 16000.0)\n",
      "iteration 0 / 5000: loss 496.625174\n",
      "iteration 100 / 5000: loss 296.931132\n",
      "iteration 200 / 5000: loss 177.984585\n",
      "iteration 300 / 5000: loss 107.143155\n",
      "iteration 400 / 5000: loss 64.918026\n",
      "iteration 500 / 5000: loss 39.617362\n",
      "iteration 600 / 5000: loss 24.568943\n",
      "iteration 700 / 5000: loss 15.428804\n",
      "iteration 800 / 5000: loss 10.050025\n",
      "iteration 900 / 5000: loss 6.858220\n",
      "iteration 1000 / 5000: loss 4.906950\n",
      "iteration 1100 / 5000: loss 3.744139\n",
      "iteration 1200 / 5000: loss 3.032787\n",
      "iteration 1300 / 5000: loss 2.675971\n",
      "iteration 1400 / 5000: loss 2.375125\n",
      "iteration 1500 / 5000: loss 2.219387\n",
      "iteration 1600 / 5000: loss 2.152684\n",
      "iteration 1700 / 5000: loss 2.107856\n",
      "iteration 1800 / 5000: loss 2.024448\n",
      "iteration 1900 / 5000: loss 2.096818\n",
      "iteration 2000 / 5000: loss 2.122052\n",
      "iteration 2100 / 5000: loss 1.994789\n",
      "iteration 2200 / 5000: loss 2.029590\n",
      "iteration 2300 / 5000: loss 2.038187\n",
      "iteration 2400 / 5000: loss 2.036784\n",
      "iteration 2500 / 5000: loss 2.073255\n",
      "iteration 2600 / 5000: loss 2.018908\n",
      "iteration 2700 / 5000: loss 2.018277\n",
      "iteration 2800 / 5000: loss 1.970061\n",
      "iteration 2900 / 5000: loss 2.068096\n",
      "iteration 3000 / 5000: loss 2.075835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3100 / 5000: loss 2.010186\n",
      "iteration 3200 / 5000: loss 2.101252\n",
      "iteration 3300 / 5000: loss 2.028766\n",
      "iteration 3400 / 5000: loss 1.992048\n",
      "iteration 3500 / 5000: loss 2.072639\n",
      "iteration 3600 / 5000: loss 2.047093\n",
      "iteration 3700 / 5000: loss 2.098082\n",
      "iteration 3800 / 5000: loss 2.075998\n",
      "iteration 3900 / 5000: loss 2.097869\n",
      "iteration 4000 / 5000: loss 2.048841\n",
      "iteration 4100 / 5000: loss 2.060217\n",
      "iteration 4200 / 5000: loss 2.035885\n",
      "iteration 4300 / 5000: loss 2.045214\n",
      "iteration 4400 / 5000: loss 2.093992\n",
      "iteration 4500 / 5000: loss 2.042367\n",
      "iteration 4600 / 5000: loss 2.016917\n",
      "iteration 4700 / 5000: loss 2.007233\n",
      "iteration 4800 / 5000: loss 2.058327\n",
      "iteration 4900 / 5000: loss 2.087389\n",
      "(8e-08, 18000.0)\n",
      "iteration 0 / 5000: loss 552.052179\n",
      "iteration 100 / 5000: loss 309.508734\n",
      "iteration 200 / 5000: loss 174.248991\n",
      "iteration 300 / 5000: loss 98.493196\n",
      "iteration 400 / 5000: loss 56.081670\n",
      "iteration 500 / 5000: loss 32.322446\n",
      "iteration 600 / 5000: loss 19.057251\n",
      "iteration 700 / 5000: loss 11.533506\n",
      "iteration 800 / 5000: loss 7.423504\n",
      "iteration 900 / 5000: loss 5.084066\n",
      "iteration 1000 / 5000: loss 3.740923\n",
      "iteration 1100 / 5000: loss 2.976524\n",
      "iteration 1200 / 5000: loss 2.590224\n",
      "iteration 1300 / 5000: loss 2.338854\n",
      "iteration 1400 / 5000: loss 2.236217\n",
      "iteration 1500 / 5000: loss 2.137552\n",
      "iteration 1600 / 5000: loss 2.141821\n",
      "iteration 1700 / 5000: loss 2.060741\n",
      "iteration 1800 / 5000: loss 2.039549\n",
      "iteration 1900 / 5000: loss 2.049941\n",
      "iteration 2000 / 5000: loss 2.048522\n",
      "iteration 2100 / 5000: loss 2.037058\n",
      "iteration 2200 / 5000: loss 2.112274\n",
      "iteration 2300 / 5000: loss 2.112775\n",
      "iteration 2400 / 5000: loss 2.064671\n",
      "iteration 2500 / 5000: loss 2.019479\n",
      "iteration 2600 / 5000: loss 2.075746\n",
      "iteration 2700 / 5000: loss 2.030938\n",
      "iteration 2800 / 5000: loss 2.017689\n",
      "iteration 2900 / 5000: loss 2.078511\n",
      "iteration 3000 / 5000: loss 2.071866\n",
      "iteration 3100 / 5000: loss 2.081021\n",
      "iteration 3200 / 5000: loss 2.032254\n",
      "iteration 3300 / 5000: loss 2.024553\n",
      "iteration 3400 / 5000: loss 2.035910\n",
      "iteration 3500 / 5000: loss 2.042397\n",
      "iteration 3600 / 5000: loss 1.960444\n",
      "iteration 3700 / 5000: loss 2.088065\n",
      "iteration 3800 / 5000: loss 2.030420\n",
      "iteration 3900 / 5000: loss 2.046966\n",
      "iteration 4000 / 5000: loss 2.066698\n",
      "iteration 4100 / 5000: loss 1.998974\n",
      "iteration 4200 / 5000: loss 2.078535\n",
      "iteration 4300 / 5000: loss 2.053126\n",
      "iteration 4400 / 5000: loss 2.047702\n",
      "iteration 4500 / 5000: loss 2.060412\n",
      "iteration 4600 / 5000: loss 2.066115\n",
      "iteration 4700 / 5000: loss 2.029170\n",
      "iteration 4800 / 5000: loss 2.048298\n",
      "iteration 4900 / 5000: loss 2.032548\n",
      "(8e-08, 20000.0)\n",
      "iteration 0 / 5000: loss 627.057812\n",
      "iteration 100 / 5000: loss 330.491251\n",
      "iteration 200 / 5000: loss 174.418416\n",
      "iteration 300 / 5000: loss 92.690393\n",
      "iteration 400 / 5000: loss 49.679636\n",
      "iteration 500 / 5000: loss 27.089854\n",
      "iteration 600 / 5000: loss 15.250983\n",
      "iteration 700 / 5000: loss 9.061580\n",
      "iteration 800 / 5000: loss 5.665355\n",
      "iteration 900 / 5000: loss 3.964086\n",
      "iteration 1000 / 5000: loss 3.139576\n",
      "iteration 1100 / 5000: loss 2.561514\n",
      "iteration 1200 / 5000: loss 2.344283\n",
      "iteration 1300 / 5000: loss 2.244699\n",
      "iteration 1400 / 5000: loss 2.234638\n",
      "iteration 1500 / 5000: loss 2.081354\n",
      "iteration 1600 / 5000: loss 2.071807\n",
      "iteration 1700 / 5000: loss 2.128843\n",
      "iteration 1800 / 5000: loss 2.064806\n",
      "iteration 1900 / 5000: loss 2.017594\n",
      "iteration 2000 / 5000: loss 2.072814\n",
      "iteration 2100 / 5000: loss 2.079113\n",
      "iteration 2200 / 5000: loss 2.103977\n",
      "iteration 2300 / 5000: loss 2.023908\n",
      "iteration 2400 / 5000: loss 2.046306\n",
      "iteration 2500 / 5000: loss 2.029040\n",
      "iteration 2600 / 5000: loss 2.074391\n",
      "iteration 2700 / 5000: loss 2.022604\n",
      "iteration 2800 / 5000: loss 2.032371\n",
      "iteration 2900 / 5000: loss 2.082965\n",
      "iteration 3000 / 5000: loss 2.033982\n",
      "iteration 3100 / 5000: loss 2.057903\n",
      "iteration 3200 / 5000: loss 2.022267\n",
      "iteration 3300 / 5000: loss 2.004403\n",
      "iteration 3400 / 5000: loss 2.084705\n",
      "iteration 3500 / 5000: loss 2.126357\n",
      "iteration 3600 / 5000: loss 2.084434\n",
      "iteration 3700 / 5000: loss 2.024055\n",
      "iteration 3800 / 5000: loss 2.096887\n",
      "iteration 3900 / 5000: loss 2.047193\n",
      "iteration 4000 / 5000: loss 2.053563\n",
      "iteration 4100 / 5000: loss 2.096208\n",
      "iteration 4200 / 5000: loss 2.118634\n",
      "iteration 4300 / 5000: loss 2.034050\n",
      "iteration 4400 / 5000: loss 2.049953\n",
      "iteration 4500 / 5000: loss 2.079035\n",
      "iteration 4600 / 5000: loss 2.077852\n",
      "iteration 4700 / 5000: loss 2.050845\n",
      "iteration 4800 / 5000: loss 2.058080\n",
      "iteration 4900 / 5000: loss 2.059948\n",
      "(8e-08, 23000.0)\n",
      "iteration 0 / 5000: loss 712.923233\n",
      "iteration 100 / 5000: loss 340.727317\n",
      "iteration 200 / 5000: loss 163.864457\n",
      "iteration 300 / 5000: loss 79.250222\n",
      "iteration 400 / 5000: loss 38.988839\n",
      "iteration 500 / 5000: loss 19.605044\n",
      "iteration 600 / 5000: loss 10.495234\n",
      "iteration 700 / 5000: loss 6.074162\n",
      "iteration 800 / 5000: loss 3.920060\n",
      "iteration 900 / 5000: loss 2.972104\n",
      "iteration 1000 / 5000: loss 2.471456\n",
      "iteration 1100 / 5000: loss 2.257842\n",
      "iteration 1200 / 5000: loss 2.165950\n",
      "iteration 1300 / 5000: loss 2.144609\n",
      "iteration 1400 / 5000: loss 2.061707\n",
      "iteration 1500 / 5000: loss 2.074567\n",
      "iteration 1600 / 5000: loss 2.062385\n",
      "iteration 1700 / 5000: loss 2.098634\n",
      "iteration 1800 / 5000: loss 2.083338\n",
      "iteration 1900 / 5000: loss 2.087267\n",
      "iteration 2000 / 5000: loss 2.050368\n",
      "iteration 2100 / 5000: loss 2.047220\n",
      "iteration 2200 / 5000: loss 2.081310\n",
      "iteration 2300 / 5000: loss 2.031270\n",
      "iteration 2400 / 5000: loss 2.067338\n",
      "iteration 2500 / 5000: loss 2.075790\n",
      "iteration 2600 / 5000: loss 2.101795\n",
      "iteration 2700 / 5000: loss 2.051474\n",
      "iteration 2800 / 5000: loss 2.067730\n",
      "iteration 2900 / 5000: loss 2.030210\n",
      "iteration 3000 / 5000: loss 2.126812\n",
      "iteration 3100 / 5000: loss 2.071155\n",
      "iteration 3200 / 5000: loss 2.097016\n",
      "iteration 3300 / 5000: loss 2.132459\n",
      "iteration 3400 / 5000: loss 2.092682\n",
      "iteration 3500 / 5000: loss 2.119806\n",
      "iteration 3600 / 5000: loss 2.144694\n",
      "iteration 3700 / 5000: loss 2.094857\n",
      "iteration 3800 / 5000: loss 2.047441\n",
      "iteration 3900 / 5000: loss 2.117442\n",
      "iteration 4000 / 5000: loss 2.027279\n",
      "iteration 4100 / 5000: loss 2.091018\n",
      "iteration 4200 / 5000: loss 2.072128\n",
      "iteration 4300 / 5000: loss 2.079102\n",
      "iteration 4400 / 5000: loss 2.094957\n",
      "iteration 4500 / 5000: loss 2.070821\n",
      "iteration 4600 / 5000: loss 2.085487\n",
      "iteration 4700 / 5000: loss 2.114724\n",
      "iteration 4800 / 5000: loss 2.049978\n",
      "iteration 4900 / 5000: loss 2.040409\n",
      "(8e-08, 26000.0)\n",
      "iteration 0 / 5000: loss 804.458030\n",
      "iteration 100 / 5000: loss 348.944765\n",
      "iteration 200 / 5000: loss 152.209064\n",
      "iteration 300 / 5000: loss 67.193041\n",
      "iteration 400 / 5000: loss 30.295727\n",
      "iteration 500 / 5000: loss 14.291607\n",
      "iteration 600 / 5000: loss 7.401091\n",
      "iteration 700 / 5000: loss 4.371287\n",
      "iteration 800 / 5000: loss 3.086494\n",
      "iteration 900 / 5000: loss 2.570090\n",
      "iteration 1000 / 5000: loss 2.275633\n",
      "iteration 1100 / 5000: loss 2.189477\n",
      "iteration 1200 / 5000: loss 2.171822\n",
      "iteration 1300 / 5000: loss 2.084545\n",
      "iteration 1400 / 5000: loss 2.169695\n",
      "iteration 1500 / 5000: loss 2.101344\n",
      "iteration 1600 / 5000: loss 2.053835\n",
      "iteration 1700 / 5000: loss 2.081010\n",
      "iteration 1800 / 5000: loss 2.098687\n",
      "iteration 1900 / 5000: loss 2.084073\n",
      "iteration 2000 / 5000: loss 2.065146\n",
      "iteration 2100 / 5000: loss 2.117228\n",
      "iteration 2200 / 5000: loss 2.087433\n",
      "iteration 2300 / 5000: loss 2.056293\n",
      "iteration 2400 / 5000: loss 2.078123\n",
      "iteration 2500 / 5000: loss 2.062619\n",
      "iteration 2600 / 5000: loss 2.151507\n",
      "iteration 2700 / 5000: loss 2.103216\n",
      "iteration 2800 / 5000: loss 2.103291\n",
      "iteration 2900 / 5000: loss 2.090929\n",
      "iteration 3000 / 5000: loss 2.120466\n",
      "iteration 3100 / 5000: loss 2.085802\n",
      "iteration 3200 / 5000: loss 2.058574\n",
      "iteration 3300 / 5000: loss 2.019778\n",
      "iteration 3400 / 5000: loss 2.160057\n",
      "iteration 3500 / 5000: loss 2.076960\n",
      "iteration 3600 / 5000: loss 2.075755\n",
      "iteration 3700 / 5000: loss 2.122835\n",
      "iteration 3800 / 5000: loss 2.101042\n",
      "iteration 3900 / 5000: loss 2.028389\n",
      "iteration 4000 / 5000: loss 2.110728\n",
      "iteration 4100 / 5000: loss 2.071240\n",
      "iteration 4200 / 5000: loss 2.155579\n",
      "iteration 4300 / 5000: loss 2.073648\n",
      "iteration 4400 / 5000: loss 2.082471\n",
      "iteration 4500 / 5000: loss 2.100056\n",
      "iteration 4600 / 5000: loss 2.060895\n",
      "iteration 4700 / 5000: loss 2.075946\n",
      "iteration 4800 / 5000: loss 2.099771\n",
      "iteration 4900 / 5000: loss 2.081200\n",
      "lr 2.000000e-08 reg 1.600000e+04 train accuracy: 0.340735 val accuracy: 0.349000\n",
      "lr 2.000000e-08 reg 1.800000e+04 train accuracy: 0.338776 val accuracy: 0.355000\n",
      "lr 2.000000e-08 reg 2.000000e+04 train accuracy: 0.334204 val accuracy: 0.350000\n",
      "lr 2.000000e-08 reg 2.300000e+04 train accuracy: 0.330898 val accuracy: 0.342000\n",
      "lr 2.000000e-08 reg 2.600000e+04 train accuracy: 0.331000 val accuracy: 0.343000\n",
      "lr 3.000000e-08 reg 1.600000e+04 train accuracy: 0.343469 val accuracy: 0.357000\n",
      "lr 3.000000e-08 reg 1.800000e+04 train accuracy: 0.339306 val accuracy: 0.356000\n",
      "lr 3.000000e-08 reg 2.000000e+04 train accuracy: 0.336122 val accuracy: 0.351000\n",
      "lr 3.000000e-08 reg 2.300000e+04 train accuracy: 0.331020 val accuracy: 0.346000\n",
      "lr 3.000000e-08 reg 2.600000e+04 train accuracy: 0.333796 val accuracy: 0.355000\n",
      "lr 4.000000e-08 reg 1.600000e+04 train accuracy: 0.344327 val accuracy: 0.360000\n",
      "lr 4.000000e-08 reg 1.800000e+04 train accuracy: 0.341449 val accuracy: 0.359000\n",
      "lr 4.000000e-08 reg 2.000000e+04 train accuracy: 0.336796 val accuracy: 0.357000\n",
      "lr 4.000000e-08 reg 2.300000e+04 train accuracy: 0.330204 val accuracy: 0.339000\n",
      "lr 4.000000e-08 reg 2.600000e+04 train accuracy: 0.325061 val accuracy: 0.340000\n",
      "lr 5.000000e-08 reg 1.600000e+04 train accuracy: 0.341898 val accuracy: 0.362000\n",
      "lr 5.000000e-08 reg 1.800000e+04 train accuracy: 0.338367 val accuracy: 0.356000\n",
      "lr 5.000000e-08 reg 2.000000e+04 train accuracy: 0.335286 val accuracy: 0.346000\n",
      "lr 5.000000e-08 reg 2.300000e+04 train accuracy: 0.333755 val accuracy: 0.343000\n",
      "lr 5.000000e-08 reg 2.600000e+04 train accuracy: 0.325245 val accuracy: 0.342000\n",
      "lr 6.000000e-08 reg 1.600000e+04 train accuracy: 0.342041 val accuracy: 0.350000\n",
      "lr 6.000000e-08 reg 1.800000e+04 train accuracy: 0.341163 val accuracy: 0.353000\n",
      "lr 6.000000e-08 reg 2.000000e+04 train accuracy: 0.337041 val accuracy: 0.354000\n",
      "lr 6.000000e-08 reg 2.300000e+04 train accuracy: 0.332714 val accuracy: 0.345000\n",
      "lr 6.000000e-08 reg 2.600000e+04 train accuracy: 0.330633 val accuracy: 0.347000\n",
      "lr 7.000000e-08 reg 1.600000e+04 train accuracy: 0.339673 val accuracy: 0.356000\n",
      "lr 7.000000e-08 reg 1.800000e+04 train accuracy: 0.344531 val accuracy: 0.356000\n",
      "lr 7.000000e-08 reg 2.000000e+04 train accuracy: 0.336592 val accuracy: 0.343000\n",
      "lr 7.000000e-08 reg 2.300000e+04 train accuracy: 0.331449 val accuracy: 0.351000\n",
      "lr 7.000000e-08 reg 2.600000e+04 train accuracy: 0.328673 val accuracy: 0.331000\n",
      "lr 8.000000e-08 reg 1.600000e+04 train accuracy: 0.345816 val accuracy: 0.359000\n",
      "lr 8.000000e-08 reg 1.800000e+04 train accuracy: 0.339449 val accuracy: 0.352000\n",
      "lr 8.000000e-08 reg 2.000000e+04 train accuracy: 0.342184 val accuracy: 0.355000\n",
      "lr 8.000000e-08 reg 2.300000e+04 train accuracy: 0.332224 val accuracy: 0.347000\n",
      "lr 8.000000e-08 reg 2.600000e+04 train accuracy: 0.327408 val accuracy: 0.336000\n",
      "best validation accuracy achieved during cross-validation: 0.362000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [2e-8, 3e-8, 4e-8, 5e-8, 6e-8, 7e-8, 8e-8]\n",
    "regularization_strengths = [1.6e4, 1.8e4, 2e4 , 2.3e4, 2.6e4]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "num_iters = 5000\n",
    "for lr in learning_rates:\n",
    "    for reg in regularization_strengths:\n",
    "        print((lr, reg))\n",
    "        softmax = Softmax()\n",
    "#         softmax = best_softmax\n",
    "        _ = softmax.train(X_train, y_train, learning_rate=lr, reg=reg,\n",
    "                      num_iters=num_iters, verbose=True)\n",
    "        y_train_pred = softmax.predict(X_train)\n",
    "        y_val_pred = softmax.predict(X_val)\n",
    "        train_accuracy = np.mean(y_train == y_train_pred)\n",
    "        val_accuracy = np.mean(y_val == y_val_pred)\n",
    "        if val_accuracy > best_val:\n",
    "            best_val = val_accuracy\n",
    "            best_softmax = softmax\n",
    "        results[(lr, reg)] = (train_accuracy, val_accuracy)\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.360000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAF8CAYAAADrUz6WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116527198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
